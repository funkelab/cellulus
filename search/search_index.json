{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"getting-started/","title":"Installation","text":""},{"location":"getting-started/#using-the-terminal","title":"Using the terminal","text":"<p>If you would like to run <code>cellulus</code> , using GPU:</p> <pre><code>conda create -y -n cellulus python==3.9\nconda activate cellulus\nconda install pytorch==2.0.1 torchvision==0.15.2 pytorch-cuda=11.7 -c pytorch -c nvidia\ngit clone https://github.com/funkelab/cellulus.git\ncd cellulus\npip install -e .\n</code></pre> <p>If you would like to run <code>cellulus</code>, using the CPU or the MPS framework:</p> <pre><code>conda create -y -n cellulus python==3.9\nconda activate cellulus\npip install torch torchvision\ngit clone https://github.com/funkelab/cellulus.git\ncd cellulus\npip install -e .\n</code></pre>"},{"location":"getting-started/#using-the-napari-plugin","title":"Using the <code>napari</code> plugin","text":""},{"location":"api/dataset_config/","title":"DatasetConfig","text":"<p>Dataset configuration.</p>"},{"location":"api/dataset_config/#cellulus.configs.dataset_config.DatasetConfig--parameters","title":"Parameters","text":"<pre><code>container_path:\n\n    A path to the zarr/N5 container.\n\ndataset_name:\n\n    The name of the dataset containing the raw data in the container.\n\nsecondary_dataset_name:\n\n    The name of the secondary dataset containing the data which needs\n    processing.\n\n'dataset_name' and 'secondary_dataset_name' can be thought of as the\noutput and input to a certain task, respectively.\nFor example, during segmentation, 'dataset_name' would refer to the output\nsegmentation masks and 'secondary_dataset_name' would refer to the input\npredicted embeddings.\nDuring evaluation, 'dataset_name' would refer to the ground truth masks\nand 'secondary_dataset_name' would refer to the input segmentation masks.\n</code></pre> Source code in <code>cellulus/configs/dataset_config.py</code> <pre><code>@attrs.define\nclass DatasetConfig:\n    \"\"\"Dataset configuration.\n\n    Parameters\n    ----------\n\n        container_path:\n\n            A path to the zarr/N5 container.\n\n        dataset_name:\n\n            The name of the dataset containing the raw data in the container.\n\n        secondary_dataset_name:\n\n            The name of the secondary dataset containing the data which needs\n            processing.\n\n        'dataset_name' and 'secondary_dataset_name' can be thought of as the\n        output and input to a certain task, respectively.\n        For example, during segmentation, 'dataset_name' would refer to the output\n        segmentation masks and 'secondary_dataset_name' would refer to the input\n        predicted embeddings.\n        During evaluation, 'dataset_name' would refer to the ground truth masks\n        and 'secondary_dataset_name' would refer to the input segmentation masks.\n\n    \"\"\"\n\n    container_path: Path = attrs.field(converter=Path)\n    dataset_name: str = attrs.field(validator=instance_of(str))\n    secondary_dataset_name: str = attrs.field(\n        default=None, validator=optional(instance_of(str))\n    )\n</code></pre>"},{"location":"api/experiment_config/","title":"ExperimentConfig","text":"<p>Top-level config for an experiment (containing training and prediction).</p>"},{"location":"api/experiment_config/#cellulus.configs.experiment_config.ExperimentConfig--parameters","title":"Parameters","text":"<pre><code>experiment_name: (default = 'YYYY-MM-DD')\n\n    A unique name for the experiment.\n\nobject_size: (default = 30)\n\n    A rough estimate of the size of objects in the image, given in\n    world units. The \"patch size\" of the network will be chosen based\n    on this estimate.\n\nnormalization_factor: (default = None)\n\n    The factor to use, for dividing the raw image pixel intensities.\n    If 'None', a factor is chosen based on the dtype of the array .\n    (e.g., np.uint8 would result in a factor of 1.0/255).\n\nmodel_config:\n\n    Configuration object for the model.\n\ntrain_config:\n\n    Configuration object for training.\n\ninference_config:\n\n    Configuration object for prediction.\n</code></pre> Source code in <code>cellulus/configs/experiment_config.py</code> <pre><code>@attrs.define\nclass ExperimentConfig:\n    \"\"\"Top-level config for an experiment (containing training and prediction).\n\n    Parameters\n    ----------\n\n        experiment_name: (default = 'YYYY-MM-DD')\n\n            A unique name for the experiment.\n\n        object_size: (default = 30)\n\n            A rough estimate of the size of objects in the image, given in\n            world units. The \"patch size\" of the network will be chosen based\n            on this estimate.\n\n        normalization_factor: (default = None)\n\n            The factor to use, for dividing the raw image pixel intensities.\n            If 'None', a factor is chosen based on the dtype of the array .\n            (e.g., np.uint8 would result in a factor of 1.0/255).\n\n        model_config:\n\n            Configuration object for the model.\n\n        train_config:\n\n            Configuration object for training.\n\n        inference_config:\n\n            Configuration object for prediction.\n    \"\"\"\n\n    model_config: ModelConfig = attrs.field(converter=to_config(ModelConfig))\n    experiment_name: str = attrs.field(\n        default=datetime.today().strftime(\"%Y-%m-%d\"), validator=instance_of(str)\n    )\n    normalization_factor: float = attrs.field(\n        default=None, validator=attrs.validators.optional(instance_of(float))\n    )\n    object_size: int = attrs.field(default=30, validator=instance_of(int))\n\n    train_config: TrainConfig = attrs.field(\n        default=None, converter=to_config(TrainConfig)\n    )\n    inference_config: InferenceConfig = attrs.field(\n        default=None, converter=to_config(InferenceConfig)\n    )\n</code></pre>"},{"location":"api/inference_config/","title":"InferenceConfig","text":"<p>Inference configuration.</p>"},{"location":"api/inference_config/#cellulus.configs.inference_config.InferenceConfig--parameters","title":"Parameters","text":"<pre><code>dataset_config:\n\n    Configuration object for the data to predict and segment.\n\nprediction_dataset_config:\n\n    Configuration object produced by predict.py.\n\ndetection_dataset_config:\n\n    Configuration object produced by detect.py.\n\nsegmentation_dataset_config:\n\n    Configuration object produced by segment.py.\n\nevaluation_dataset_config:\n\n    Configuration object for the ground truth masks.\n\ncrop_size (default = [252, 252]):\n\n    ROI used by the scan node in gunpowder.\n\np_salt_pepper (default = 0.01):\n\n    Fraction of pixels that will have salt-pepper noise.\n\nnum_infer_iterations (default = 16):\n\n    Number of times the salt-peper noise is added to the raw image.\n    This is used to infer the foreground and background in the raw image.\n\nbandwidth (default = None):\n\n    Bandwidth used to perform mean-shift clustering on the predicted\n    embeddings.\n\nthreshold (default = None):\n\n    Threshold to use for binary partitioning into foreground and background\n    pixel regions. If None, this is figured out automatically by performing\n    Otsu Thresholding on the last channel of the predicted embeddings.\n\nmin_size (default = None):\n\n    Ignore objects which are smaller than `min_size` number of pixels.\n\ndevice (default = 'cuda:0'):\n\n    The device to infer on.\n    Set to 'cpu' to infer without GPU.\n\nclustering (default = 'meanshift'):\n\n    How to cluster the embeddings?\n    Can be one of 'meanshift' or 'greedy'.\n\nuse_seeds (default = False):\n\n    If set to True, the local optima of the distance map from the\n    predicted object centers is used.\n    Else, seeds are determined by sklearn.cluster.MeanShift.\n\nnum_bandwidths (default = 1):\n\n    Number of bandwidths to obtain segmentations for.\n\nreduction_probability (default = 0.1):\n\n    If set to less than 1.0, this fraction of available pixels are used\n    to determine the clusters (fitting stage) while performing\n    meanshift clustering.\n    Once clusters are available, they are used to predict the cluster assignment\n    of the remaining pixels (prediction stage).\n\nmin_size (default = None):\n\n    Objects below `min_size` pixels will be removed.\n\npost_processing (default= 'morphological'):\n\n    Can be one of 'morphological' or 'intensity' operations.\n    If 'morphological', the individual detections grow and shrink by\n    'grow_distance' and 'shrink_distance' number of pixels.\n\n    If 'intensity', each detection is partitioned based on a binary intensity\n    threshold calculated automatically from the raw image data.\n    By default, the channel `0` in the raw image is used for\n    intensity thresholding.\n\ngrow_distance (default = 3):\n\n    Only used if post_processing (see above) is equal to 'morphological'.\n\nshrink_distance (default = 6):\n\n    Only used if post_processing (see above) is equal to\n    'morphological'.\n</code></pre> Source code in <code>cellulus/configs/inference_config.py</code> <pre><code>@attrs.define\nclass InferenceConfig:\n    \"\"\"Inference configuration.\n\n    Parameters\n    ----------\n\n        dataset_config:\n\n            Configuration object for the data to predict and segment.\n\n        prediction_dataset_config:\n\n            Configuration object produced by predict.py.\n\n        detection_dataset_config:\n\n            Configuration object produced by detect.py.\n\n        segmentation_dataset_config:\n\n            Configuration object produced by segment.py.\n\n        evaluation_dataset_config:\n\n            Configuration object for the ground truth masks.\n\n        crop_size (default = [252, 252]):\n\n            ROI used by the scan node in gunpowder.\n\n        p_salt_pepper (default = 0.01):\n\n            Fraction of pixels that will have salt-pepper noise.\n\n        num_infer_iterations (default = 16):\n\n            Number of times the salt-peper noise is added to the raw image.\n            This is used to infer the foreground and background in the raw image.\n\n        bandwidth (default = None):\n\n            Bandwidth used to perform mean-shift clustering on the predicted\n            embeddings.\n\n        threshold (default = None):\n\n            Threshold to use for binary partitioning into foreground and background\n            pixel regions. If None, this is figured out automatically by performing\n            Otsu Thresholding on the last channel of the predicted embeddings.\n\n        min_size (default = None):\n\n            Ignore objects which are smaller than `min_size` number of pixels.\n\n        device (default = 'cuda:0'):\n\n            The device to infer on.\n            Set to 'cpu' to infer without GPU.\n\n        clustering (default = 'meanshift'):\n\n            How to cluster the embeddings?\n            Can be one of 'meanshift' or 'greedy'.\n\n        use_seeds (default = False):\n\n            If set to True, the local optima of the distance map from the\n            predicted object centers is used.\n            Else, seeds are determined by sklearn.cluster.MeanShift.\n\n        num_bandwidths (default = 1):\n\n            Number of bandwidths to obtain segmentations for.\n\n        reduction_probability (default = 0.1):\n\n            If set to less than 1.0, this fraction of available pixels are used\n            to determine the clusters (fitting stage) while performing\n            meanshift clustering.\n            Once clusters are available, they are used to predict the cluster assignment\n            of the remaining pixels (prediction stage).\n\n        min_size (default = None):\n\n            Objects below `min_size` pixels will be removed.\n\n        post_processing (default= 'morphological'):\n\n            Can be one of 'morphological' or 'intensity' operations.\n            If 'morphological', the individual detections grow and shrink by\n            'grow_distance' and 'shrink_distance' number of pixels.\n\n            If 'intensity', each detection is partitioned based on a binary intensity\n            threshold calculated automatically from the raw image data.\n            By default, the channel `0` in the raw image is used for\n            intensity thresholding.\n\n        grow_distance (default = 3):\n\n            Only used if post_processing (see above) is equal to 'morphological'.\n\n        shrink_distance (default = 6):\n\n            Only used if post_processing (see above) is equal to\n            'morphological'.\n\n    \"\"\"\n\n    dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n\n    prediction_dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n\n    detection_dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n\n    segmentation_dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n\n    evaluation_dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n    device: str = attrs.field(default=\"cuda:0\", validator=instance_of(str))\n    crop_size: List = attrs.field(default=[252, 252], validator=instance_of(List))\n    p_salt_pepper = attrs.field(default=0.01, validator=instance_of(float))\n    num_infer_iterations = attrs.field(default=16, validator=instance_of(int))\n    threshold = attrs.field(\n        default=None, validator=attrs.validators.optional(instance_of(float))\n    )\n    clustering = attrs.field(\n        default=\"meanshift\", validator=in_([\"meanshift\", \"greedy\"])\n    )\n    use_seeds = attrs.field(default=False, validator=instance_of(bool))\n    bandwidth = attrs.field(\n        default=None, validator=attrs.validators.optional(instance_of(float))\n    )\n    num_bandwidths = attrs.field(default=1, validator=instance_of(int))\n    reduction_probability = attrs.field(default=0.1, validator=instance_of(float))\n    min_size = attrs.field(\n        default=None, validator=attrs.validators.optional(instance_of(int))\n    )\n    post_processing = attrs.field(default=\"cell\", validator=in_([\"cell\", \"nucleus\"]))\n    grow_distance = attrs.field(default=3, validator=instance_of(int))\n    shrink_distance = attrs.field(default=6, validator=instance_of(int))\n</code></pre>"},{"location":"api/meta_data/","title":"DatasetMetaData","text":"Source code in <code>cellulus/datasets/meta_data.py</code> <pre><code>class DatasetMetaData:\n    def __init__(self, shape, axis_names):\n        self.num_dims = len(axis_names)\n        self.num_spatial_dims: int = 0\n        self.num_samples: int = 0\n        self.num_channels: int = 0\n        self.sample_dim = None\n        self.channel_dim = None\n        self.time_dim = None\n        self.spatial_array: Tuple[int, ...] = ()\n        for dim, axis_name in enumerate(axis_names):\n            if axis_name == \"s\":\n                self.sample_dim = dim\n                self.num_samples = shape[dim]\n            elif axis_name == \"c\":\n                self.channel_dim = dim\n                self.num_channels = shape[dim]\n            elif axis_name == \"t\":\n                self.num_spatial_dims += 1\n                self.time_dim = dim\n            elif axis_name == \"z\":\n                self.num_spatial_dims += 1\n                self.spatial_array += (shape[dim],)\n            elif axis_name == \"y\":\n                self.num_spatial_dims += 1\n                self.spatial_array += (shape[dim],)\n            elif axis_name == \"x\":\n                self.num_spatial_dims += 1\n                self.spatial_array += (shape[dim],)\n\n        if self.sample_dim is None:\n            self.__invalid_dataset(\"dataset does not have a sample dimension\")\n\n        if self.channel_dim is None:\n            self.__invalid_dataset(\"dataset does not have a channel dimension\")\n\n        if self.num_dims != len(shape):\n            self.__invalid_dataset(\n                f\"dataset has {len(shape)} dimensions, but attribute \"\n                f\"axis_names has {self.num_dims} entries\"\n            )\n\n    @staticmethod\n    def from_dataset_config(dataset_config: DatasetConfig) -&gt; \"DatasetMetaData\":\n        container = zarr.open(dataset_config.container_path, \"r\")\n        try:\n            data = container[dataset_config.dataset_name]\n        except KeyError:\n            DatasetMetaData.__invalid_dataset(\n                f\"Zarr container {dataset_config.container_path} does not contain \"\n                f'\"{dataset_config.dataset_name}\" dataset'\n            )\n\n        try:\n            axis_names = data.attrs[\"axis_names\"]\n        except KeyError:\n            DatasetMetaData.__invalid_dataset(\n                f'\"{dataset_config.dataset_name}\" dataset in '\n                f'{dataset_config.container_path} does not contain \"axis_names\" '\n                \"attribute\"\n            )\n\n        try:\n            return DatasetMetaData(data.shape, axis_names)\n        except RuntimeError as e:\n            raise RuntimeError(\n                f'\"{dataset_config.dataset_name}\" dataset in '\n                f\"{dataset_config.container_path} has invalid meta-data\"\n            ) from e\n\n    @staticmethod\n    def __invalid_dataset(message):\n        raise RuntimeError(\n            message\n            + \"\\n\\n\"\n            + (\n                \"The raw dataset should have shape \"\n                \"(s, c, [t,] [z,] y, x), where s = # of samples, c = # of channels, \"\n                \"t = # of frames, and z/y/x are spatial extents. The dataset should \"\n                'have an \"axis_names\" attribute that contains the names of the used '\n                'axes, e.g., [\"s\", \"c\", \"y\", \"x\"] for a 2D dataset.'\n            )\n        )\n</code></pre>"},{"location":"api/model_config/","title":"ModelConfig","text":"<p>Model Configuration.</p>"},{"location":"api/model_config/#cellulus.configs.model_config.ModelConfig--parameters","title":"Parameters","text":"<pre><code>num_fmaps:\n\n    The number of feature maps in the first level of the U-Net.\n\nfmap_inc_factor:\n\n    The factor by which to increase the number of feature maps between\n    levels of the U-Net.\n\nfeatures_in_last_layer (default = 64):\n\n    The number of feature channels in the last layer of the U-Net\n\ndownsampling_factors (default = [[2,2]]):\n\n    A list of downsampling factors, each given per dimension (e.g.,\n    [[2,2], [3,3]] would correspond to two downsample layers, one with\n    an isotropic factor of 2, and another one with 3). This parameter\n    will also determine the number of levels in the U-Net.\n\ncheckpoint (default = None):\n\n    A path to a checkpoint of the network. Needs to be set for networks\n    that are used for prediction. If set during training, the\n    checkpoint will be used to resume training, otherwise the network\n    will be trained from scratch.\n\ninitialize (default = True)\n\n    If True, initialize the model weights with Kaiming Normal.\n</code></pre> Source code in <code>cellulus/configs/model_config.py</code> <pre><code>@attrs.define\nclass ModelConfig:\n    \"\"\"Model Configuration.\n\n    Parameters\n    ----------\n\n        num_fmaps:\n\n            The number of feature maps in the first level of the U-Net.\n\n        fmap_inc_factor:\n\n            The factor by which to increase the number of feature maps between\n            levels of the U-Net.\n\n        features_in_last_layer (default = 64):\n\n            The number of feature channels in the last layer of the U-Net\n\n        downsampling_factors (default = [[2,2]]):\n\n            A list of downsampling factors, each given per dimension (e.g.,\n            [[2,2], [3,3]] would correspond to two downsample layers, one with\n            an isotropic factor of 2, and another one with 3). This parameter\n            will also determine the number of levels in the U-Net.\n\n        checkpoint (default = None):\n\n            A path to a checkpoint of the network. Needs to be set for networks\n            that are used for prediction. If set during training, the\n            checkpoint will be used to resume training, otherwise the network\n            will be trained from scratch.\n\n        initialize (default = True)\n\n            If True, initialize the model weights with Kaiming Normal.\n\n    \"\"\"\n\n    num_fmaps: int = attrs.field(validator=instance_of(int))\n    fmap_inc_factor: int = attrs.field(validator=instance_of(int))\n    features_in_last_layer: int = attrs.field(default=64)\n    downsampling_factors: List[List[int]] = attrs.field(\n        default=[\n            [2, 2],\n        ]\n    )\n    checkpoint: Path = attrs.field(default=None, converter=to_path)\n    initialize: bool = attrs.field(default=True, validator=instance_of(bool))\n</code></pre>"},{"location":"api/oce_loss/","title":"OCELoss","text":"<p>             Bases: <code>Module</code></p> Source code in <code>cellulus/criterions/oce_loss.py</code> <pre><code>class OCELoss(nn.Module):  # type: ignore\n    def __init__(\n        self,\n        temperature: float,\n        regularization_weight: float,\n        density: float,\n        num_spatial_dims: int,\n        device: torch.device,\n    ):\n        \"\"\"Class definition for loss.\n\n        Parameters\n        ----------\n\n            temperature:\n                Factor used to scale the gaussian function and control\n                the rate of damping.\n\n            regularization_weight:\n                The weight of the L2 regularizer on the object-centric embeddings.\n\n            density:\n                Determines the fraction of patches to sample per crop,\n                during training.\n\n            num_spatial_dims:\n                Should be equal to 2 for 2D and 3 for 3D.\n\n            device:\n                The device to train on.\n                Set to 'cpu' to train without GPU.\n\n        \"\"\"\n        super().__init__()\n        self.temperature = temperature\n        self.regularization_weight = regularization_weight\n        self.density = density\n        self.num_spatial_dims = num_spatial_dims\n        self.device = device\n\n    @staticmethod\n    def distance_function(embedding_0, embedding_1):\n        difference = embedding_0 - embedding_1\n        return difference.norm(2, dim=-1)\n\n    def non_linearity(self, distance):\n        return 1 - (-distance.pow(2) / self.temperature).exp()\n\n    def forward(self, anchor_embedding, reference_embedding):\n        distance = self.distance_function(\n            anchor_embedding, reference_embedding.detach()\n        )\n        non_linear_distance = self.non_linearity(distance)\n        oce_loss = non_linear_distance.sum()\n        regularization_loss = (\n            self.regularization_weight * anchor_embedding.norm(2, dim=-1).sum()\n        )\n        loss = oce_loss + regularization_loss\n        return loss, oce_loss, regularization_loss\n</code></pre>"},{"location":"api/oce_loss/#cellulus.criterions.oce_loss.OCELoss.__init__","title":"<code>__init__(temperature, regularization_weight, density, num_spatial_dims, device)</code>","text":"<p>Class definition for loss.</p>"},{"location":"api/oce_loss/#cellulus.criterions.oce_loss.OCELoss.__init__--parameters","title":"Parameters","text":"<pre><code>temperature:\n    Factor used to scale the gaussian function and control\n    the rate of damping.\n\nregularization_weight:\n    The weight of the L2 regularizer on the object-centric embeddings.\n\ndensity:\n    Determines the fraction of patches to sample per crop,\n    during training.\n\nnum_spatial_dims:\n    Should be equal to 2 for 2D and 3 for 3D.\n\ndevice:\n    The device to train on.\n    Set to 'cpu' to train without GPU.\n</code></pre> Source code in <code>cellulus/criterions/oce_loss.py</code> <pre><code>def __init__(\n    self,\n    temperature: float,\n    regularization_weight: float,\n    density: float,\n    num_spatial_dims: int,\n    device: torch.device,\n):\n    \"\"\"Class definition for loss.\n\n    Parameters\n    ----------\n\n        temperature:\n            Factor used to scale the gaussian function and control\n            the rate of damping.\n\n        regularization_weight:\n            The weight of the L2 regularizer on the object-centric embeddings.\n\n        density:\n            Determines the fraction of patches to sample per crop,\n            during training.\n\n        num_spatial_dims:\n            Should be equal to 2 for 2D and 3 for 3D.\n\n        device:\n            The device to train on.\n            Set to 'cpu' to train without GPU.\n\n    \"\"\"\n    super().__init__()\n    self.temperature = temperature\n    self.regularization_weight = regularization_weight\n    self.density = density\n    self.num_spatial_dims = num_spatial_dims\n    self.device = device\n</code></pre>"},{"location":"api/train_config/","title":"TrainConfig","text":"<p>Train configuration.</p>"},{"location":"api/train_config/#cellulus.configs.train_config.TrainConfig--parameters","title":"Parameters","text":"<pre><code>crop_size (default = [252, 252]):\n\n    The size of the crops - specified as a list of number of pixels -\n    extracted from the raw images, used during training.\n\nbatch_size (default = 8):\n\n    The number of samples to use per batch.\n\nmax_iterations (default = 100000):\n\n    The maximum number of iterations to train for.\n\ninitial_learning_rate (default = 4e-5):\n\n    Initial learning rate of the optimizer.\n\ntemperature (default = 10):\n\n    Factor used to scale the gaussian function and control the rate of damping.\n\nregularizer_weight (default = 1e-5):\n\n    The weight of the L2 regularizer on the object-centric embeddings.\n\nreduce_mean (default = True):\n\n    If True, the loss contribution is averaged across all pairs of patches.\n\ndensity (default = 0.1)\n\n    Determines the fraction of patches to sample per crop, during training.\n\nkappa (default = 10.0):\n\n    Neighborhood radius to extract patches from.\n\nsave_model_every (default = 1000):\n\n    The model weights are saved every few iterations.\n\nsave_best_model_every (default = 100):\n\n    The best loss is evaluated every few iterations.\n\nsave_snapshot_every (default = 1000):\n\n    The zarr snapshot is saved every few iterations.\n\nnum_workers (default = 8):\n\n    The number of sub-processes to use for data-loading.\n\nelastic_deform (default = True):\n\n    If set to True, the data is elastically deformed\n    in order to increase training samples.\n\ncontrol_point_spacing (default = 64):\n\n    The distance in pixels between control points used for elastic\n    deformation of the raw data during training.\n    Only used if `elastic_deform` is set to True.\n\ncontrol_point_jitter (default = 2.0):\n\n    How much to jitter the control points for elastic deformation\n    of the raw data during training, given as the standard deviation of\n    a normal distribution with zero mean.\n    Only used if `elastic_deform` is set to True.\n\ntrain_data_config:\n\n    Configuration object for the training data.\n\nvalidate_data_config (default = None):\n\n    Configuration object for the validation data.\n\ndevice (default = 'cuda:0'):\n\n    The device to train on.\n    Set to 'cpu' to train without GPU.\n</code></pre> Source code in <code>cellulus/configs/train_config.py</code> <pre><code>@attrs.define\nclass TrainConfig:\n    \"\"\"Train configuration.\n\n    Parameters\n    ----------\n\n        crop_size (default = [252, 252]):\n\n            The size of the crops - specified as a list of number of pixels -\n            extracted from the raw images, used during training.\n\n        batch_size (default = 8):\n\n            The number of samples to use per batch.\n\n        max_iterations (default = 100000):\n\n            The maximum number of iterations to train for.\n\n        initial_learning_rate (default = 4e-5):\n\n            Initial learning rate of the optimizer.\n\n        temperature (default = 10):\n\n            Factor used to scale the gaussian function and control the rate of damping.\n\n        regularizer_weight (default = 1e-5):\n\n            The weight of the L2 regularizer on the object-centric embeddings.\n\n        reduce_mean (default = True):\n\n            If True, the loss contribution is averaged across all pairs of patches.\n\n        density (default = 0.1)\n\n            Determines the fraction of patches to sample per crop, during training.\n\n        kappa (default = 10.0):\n\n            Neighborhood radius to extract patches from.\n\n        save_model_every (default = 1000):\n\n            The model weights are saved every few iterations.\n\n        save_best_model_every (default = 100):\n\n            The best loss is evaluated every few iterations.\n\n        save_snapshot_every (default = 1000):\n\n            The zarr snapshot is saved every few iterations.\n\n        num_workers (default = 8):\n\n            The number of sub-processes to use for data-loading.\n\n        elastic_deform (default = True):\n\n            If set to True, the data is elastically deformed\n            in order to increase training samples.\n\n        control_point_spacing (default = 64):\n\n            The distance in pixels between control points used for elastic\n            deformation of the raw data during training.\n            Only used if `elastic_deform` is set to True.\n\n        control_point_jitter (default = 2.0):\n\n            How much to jitter the control points for elastic deformation\n            of the raw data during training, given as the standard deviation of\n            a normal distribution with zero mean.\n            Only used if `elastic_deform` is set to True.\n\n        train_data_config:\n\n            Configuration object for the training data.\n\n        validate_data_config (default = None):\n\n            Configuration object for the validation data.\n\n        device (default = 'cuda:0'):\n\n            The device to train on.\n            Set to 'cpu' to train without GPU.\n\n\n    \"\"\"\n\n    train_data_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n    validate_data_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n    crop_size: List = attrs.field(default=[252, 252], validator=instance_of(List))\n    batch_size: int = attrs.field(default=8, validator=instance_of(int))\n    max_iterations: int = attrs.field(default=100_000, validator=instance_of(int))\n    initial_learning_rate: float = attrs.field(\n        default=4e-5, validator=instance_of(float)\n    )\n    density: float = attrs.field(default=0.1, validator=instance_of(float))\n    kappa: float = attrs.field(default=10.0, validator=instance_of(float))\n    temperature: float = attrs.field(default=10.0, validator=instance_of(float))\n    regularizer_weight: float = attrs.field(default=1e-5, validator=instance_of(float))\n    save_model_every: int = attrs.field(default=1_000, validator=instance_of(int))\n    save_best_model_every: int = attrs.field(default=100, validator=instance_of(int))\n    save_snapshot_every: int = attrs.field(default=1_000, validator=instance_of(int))\n    num_workers: int = attrs.field(default=8, validator=instance_of(int))\n    elastic_deform: bool = attrs.field(default=True, validator=instance_of(bool))\n    control_point_spacing: int = attrs.field(default=64, validator=instance_of(int))\n    control_point_jitter: float = attrs.field(default=2.0, validator=instance_of(float))\n    device: str = attrs.field(default=\"cuda:0\", validator=instance_of(str))\n</code></pre>"},{"location":"api/zarr_dataset/","title":"ZarrDataset","text":"<p>             Bases: <code>IterableDataset</code></p> Source code in <code>cellulus/datasets/zarr_dataset.py</code> <pre><code>class ZarrDataset(IterableDataset):  # type: ignore\n    def __init__(\n        self,\n        dataset_config: DatasetConfig,\n        crop_size: Tuple[int, ...],\n        elastic_deform: bool,\n        control_point_spacing: int,\n        control_point_jitter: float,\n        density: float,\n        kappa: float,\n        normalization_factor: float,\n    ):\n        \"\"\"A dataset that serves random samples from a zarr container.\n\n        Args:\n\n            dataset_config:\n\n                A dataset config object pointing to the zarr dataset to use.\n                The dataset should have shape `(s, c, [t,] [z,] y, x)`, where\n                `s` = # of samples, `c` = # of channels, `t` = # of frames, and\n                `z`/`y`/`x` are spatial extents. The dataset should have an\n                `\"axis_names\"` attribute that contains the names of the used\n                axes, e.g., `[\"s\", \"c\", \"y\", \"x\"]` for a 2D dataset.\n\n\n            crop_size:\n\n                The size of data crops used during training (distinct from the\n                \"patch size\" of the method: from each crop, multiple patches\n                will be randomly selected and the loss computed on them). This\n                should be equal to the input size of the model that predicts\n                the OCEs.\n\n            elastic_deform:\n\n                Whether to elastically deform data in order to augment training samples?\n\n            control_point_spacing:\n\n                The distance in pixels between control points used for elastic\n                deformation of the raw data.\n                Only used, if `elastic_deform` is set to True.\n\n            control_point_jitter:\n\n                How much to jitter the control points for elastic deformation\n                of the raw data, given as the standard deviation of a normal\n                distribution with zero mean.\n                Only used if `elastic_deform` is set to True.\n\n            density:\n\n                Determines the fraction of patches to sample per crop, during training.\n\n            kappa:\n\n                Neighborhood radius to extract patches from.\n\n            normalization_factor:\n\n                The factor to use, for dividing the raw image pixel intensities.\n                If 'None', a factor is chosen based on the dtype of the array .\n                (e.g., np.uint8 would result in a factor of 1.0/255).\n        \"\"\"\n\n        self.dataset_config = dataset_config\n        self.crop_size = crop_size\n        self.elastic_deform = elastic_deform\n        self.control_point_spacing = control_point_spacing\n        self.control_point_jitter = control_point_jitter\n        self.normalization_factor = normalization_factor\n        self.__read_meta_data()\n\n        assert len(crop_size) == self.num_spatial_dims, (\n            f'\"crop_size\" must have the same dimension as the '\n            f'spatial(temporal) dimensions of the \"{self.dataset_config.dataset_name}\" '\n            f\"dataset which is {self.num_spatial_dims}, but it is {crop_size}\"\n        )\n        self.density = density\n        self.kappa = kappa\n        self.output_shape = tuple(int(_ - 16) for _ in self.crop_size)\n        self.normalization_factor = normalization_factor\n        self.unbiased_shape = tuple(\n            int(_ - (2 * self.kappa)) for _ in self.output_shape\n        )\n        self.__setup_pipeline()\n\n    def __iter__(self):\n        return iter(self.__yield_sample())\n\n    def __setup_pipeline(self):\n        self.raw = gp.ArrayKey(\"RAW\")\n\n        # treat all dimensions as spatial, with a voxel size of 1\n        raw_spec = gp.ArraySpec(voxel_size=(1,) * self.num_dims, interpolatable=True)\n\n        # spatial_dims = tuple(range(self.num_dims - self.num_spatial_dims,\n        # self.num_dims))\n\n        self.pipeline = (\n            gp.ZarrSource(\n                self.dataset_config.container_path,\n                {self.raw: self.dataset_config.dataset_name},\n                array_specs={self.raw: raw_spec},\n            )\n            + gp.RandomLocation()\n            + gp.Normalize(self.raw, factor=self.normalization_factor)\n        )\n\n        if self.elastic_deform:\n            self.pipeline += gp.ElasticAugment(\n                control_point_spacing=(self.control_point_spacing,)\n                * self.num_spatial_dims,\n                jitter_sigma=(self.control_point_jitter,) * self.num_spatial_dims,\n                rotation_interval=(0, math.pi / 2),\n                scale_interval=(0.9, 1.1),\n                subsample=4,\n                spatial_dims=self.num_spatial_dims,\n            )\n            # + gp.SimpleAugment(mirror_only=spatial_dims, transpose_only=spatial_dims)\n\n    def __yield_sample(self):\n        \"\"\"An infinite generator of crops.\"\"\"\n\n        with gp.build(self.pipeline):\n            while True:\n                array_is_zero = True\n                # request one sample, all channels, plus crop dimensions\n                while array_is_zero:\n                    request = gp.BatchRequest()\n                    request[self.raw] = gp.ArraySpec(\n                        roi=gp.Roi(\n                            (0,) * self.num_dims,\n                            (1, self.num_channels, *self.crop_size),\n                        )\n                    )\n\n                    sample = self.pipeline.request_batch(request)\n                    sample_data = sample[self.raw].data[0]\n                    if np.max(sample_data) &lt;= 0.0:\n                        pass\n                    else:\n                        array_is_zero = False\n                        anchor_samples, reference_samples = self.sample_coordinates()\n                yield sample_data, anchor_samples, reference_samples\n\n    def __read_meta_data(self):\n        meta_data = DatasetMetaData.from_dataset_config(self.dataset_config)\n\n        self.num_dims = meta_data.num_dims\n        self.num_spatial_dims = meta_data.num_spatial_dims\n        self.num_channels = meta_data.num_channels\n        self.num_samples = meta_data.num_samples\n        self.sample_dim = meta_data.sample_dim\n        self.channel_dim = meta_data.channel_dim\n        self.time_dim = meta_data.time_dim\n\n    def get_num_channels(self):\n        return self.num_channels\n\n    def get_num_spatial_dims(self):\n        return self.num_spatial_dims\n\n    def sample_offsets_within_radius(self, radius, number_offsets):\n        if self.num_spatial_dims == 2:\n            offsets_x = np.random.randint(-radius, radius + 1, size=2 * number_offsets)\n            offsets_y = np.random.randint(-radius, radius + 1, size=2 * number_offsets)\n            offsets_coordinates = np.stack((offsets_x, offsets_y), axis=1)\n        elif self.num_spatial_dims == 3:\n            offsets_x = np.random.randint(-radius, radius + 1, size=3 * number_offsets)\n            offsets_y = np.random.randint(-radius, radius + 1, size=3 * number_offsets)\n            offsets_z = np.random.randint(-radius, radius + 1, size=3 * number_offsets)\n            offsets_coordinates = np.stack((offsets_x, offsets_y, offsets_z), axis=1)\n\n        in_circle = (offsets_coordinates**2).sum(axis=1) &lt; radius**2\n        offsets_coordinates = offsets_coordinates[in_circle]\n        not_zero = np.absolute(offsets_coordinates).sum(axis=1) &gt; 0\n        offsets_coordinates = offsets_coordinates[not_zero]\n\n        if len(offsets_coordinates) &lt; number_offsets:\n            return self.sample_offsets_within_radius(radius, number_offsets)\n\n        return offsets_coordinates[:number_offsets]\n\n    def sample_coordinates(self):\n        num_anchors = self.get_num_anchors()\n        num_references = self.get_num_references()\n\n        if self.num_spatial_dims == 2:\n            anchor_coordinates_x = np.random.randint(\n                self.kappa,\n                self.output_shape[0] - self.kappa + 1,\n                size=num_anchors,\n            )\n            anchor_coordinates_y = np.random.randint(\n                self.kappa,\n                self.output_shape[1] - self.kappa + 1,\n                size=num_anchors,\n            )\n            anchor_coordinates = np.stack(\n                (anchor_coordinates_x, anchor_coordinates_y), axis=1\n            )\n        elif self.num_spatial_dims == 3:\n            anchor_coordinates_x = np.random.randint(\n                self.kappa,\n                self.output_shape[0] - self.kappa + 1,\n                size=num_anchors,\n            )\n            anchor_coordinates_y = np.random.randint(\n                self.kappa,\n                self.output_shape[1] - self.kappa + 1,\n                size=num_anchors,\n            )\n            anchor_coordinates_z = np.random.randint(\n                self.kappa,\n                self.output_shape[2] - self.kappa + 1,\n                size=num_anchors,\n            )\n            anchor_coordinates = np.stack(\n                (anchor_coordinates_x, anchor_coordinates_y, anchor_coordinates_z),\n                axis=1,\n            )\n        anchor_samples = np.repeat(anchor_coordinates, num_references, axis=0)\n        offset_in_pos_radius = self.sample_offsets_within_radius(\n            self.kappa, len(anchor_samples)\n        )\n        reference_samples = anchor_samples + offset_in_pos_radius\n\n        return anchor_samples, reference_samples\n\n    def get_num_anchors(self):\n        return int(self.density * self.unbiased_shape[0] * self.unbiased_shape[1])\n\n    def get_num_references(self):\n        return int(self.density * self.kappa**2 * np.pi)\n\n    def get_num_samples(self):\n        return self.get_num_anchors() * self.get_num_references()\n</code></pre>"},{"location":"api/zarr_dataset/#cellulus.datasets.zarr_dataset.ZarrDataset.__init__","title":"<code>__init__(dataset_config, crop_size, elastic_deform, control_point_spacing, control_point_jitter, density, kappa, normalization_factor)</code>","text":"<p>A dataset that serves random samples from a zarr container.</p> <p>Args:</p> <pre><code>dataset_config:\n\n    A dataset config object pointing to the zarr dataset to use.\n    The dataset should have shape `(s, c, [t,] [z,] y, x)`, where\n    `s` = # of samples, `c` = # of channels, `t` = # of frames, and\n    `z`/`y`/`x` are spatial extents. The dataset should have an\n    `\"axis_names\"` attribute that contains the names of the used\n    axes, e.g., `[\"s\", \"c\", \"y\", \"x\"]` for a 2D dataset.\n\n\ncrop_size:\n\n    The size of data crops used during training (distinct from the\n    \"patch size\" of the method: from each crop, multiple patches\n    will be randomly selected and the loss computed on them). This\n    should be equal to the input size of the model that predicts\n    the OCEs.\n\nelastic_deform:\n\n    Whether to elastically deform data in order to augment training samples?\n\ncontrol_point_spacing:\n\n    The distance in pixels between control points used for elastic\n    deformation of the raw data.\n    Only used, if `elastic_deform` is set to True.\n\ncontrol_point_jitter:\n\n    How much to jitter the control points for elastic deformation\n    of the raw data, given as the standard deviation of a normal\n    distribution with zero mean.\n    Only used if `elastic_deform` is set to True.\n\ndensity:\n\n    Determines the fraction of patches to sample per crop, during training.\n\nkappa:\n\n    Neighborhood radius to extract patches from.\n\nnormalization_factor:\n\n    The factor to use, for dividing the raw image pixel intensities.\n    If 'None', a factor is chosen based on the dtype of the array .\n    (e.g., np.uint8 would result in a factor of 1.0/255).\n</code></pre> Source code in <code>cellulus/datasets/zarr_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_config: DatasetConfig,\n    crop_size: Tuple[int, ...],\n    elastic_deform: bool,\n    control_point_spacing: int,\n    control_point_jitter: float,\n    density: float,\n    kappa: float,\n    normalization_factor: float,\n):\n    \"\"\"A dataset that serves random samples from a zarr container.\n\n    Args:\n\n        dataset_config:\n\n            A dataset config object pointing to the zarr dataset to use.\n            The dataset should have shape `(s, c, [t,] [z,] y, x)`, where\n            `s` = # of samples, `c` = # of channels, `t` = # of frames, and\n            `z`/`y`/`x` are spatial extents. The dataset should have an\n            `\"axis_names\"` attribute that contains the names of the used\n            axes, e.g., `[\"s\", \"c\", \"y\", \"x\"]` for a 2D dataset.\n\n\n        crop_size:\n\n            The size of data crops used during training (distinct from the\n            \"patch size\" of the method: from each crop, multiple patches\n            will be randomly selected and the loss computed on them). This\n            should be equal to the input size of the model that predicts\n            the OCEs.\n\n        elastic_deform:\n\n            Whether to elastically deform data in order to augment training samples?\n\n        control_point_spacing:\n\n            The distance in pixels between control points used for elastic\n            deformation of the raw data.\n            Only used, if `elastic_deform` is set to True.\n\n        control_point_jitter:\n\n            How much to jitter the control points for elastic deformation\n            of the raw data, given as the standard deviation of a normal\n            distribution with zero mean.\n            Only used if `elastic_deform` is set to True.\n\n        density:\n\n            Determines the fraction of patches to sample per crop, during training.\n\n        kappa:\n\n            Neighborhood radius to extract patches from.\n\n        normalization_factor:\n\n            The factor to use, for dividing the raw image pixel intensities.\n            If 'None', a factor is chosen based on the dtype of the array .\n            (e.g., np.uint8 would result in a factor of 1.0/255).\n    \"\"\"\n\n    self.dataset_config = dataset_config\n    self.crop_size = crop_size\n    self.elastic_deform = elastic_deform\n    self.control_point_spacing = control_point_spacing\n    self.control_point_jitter = control_point_jitter\n    self.normalization_factor = normalization_factor\n    self.__read_meta_data()\n\n    assert len(crop_size) == self.num_spatial_dims, (\n        f'\"crop_size\" must have the same dimension as the '\n        f'spatial(temporal) dimensions of the \"{self.dataset_config.dataset_name}\" '\n        f\"dataset which is {self.num_spatial_dims}, but it is {crop_size}\"\n    )\n    self.density = density\n    self.kappa = kappa\n    self.output_shape = tuple(int(_ - 16) for _ in self.crop_size)\n    self.normalization_factor = normalization_factor\n    self.unbiased_shape = tuple(\n        int(_ - (2 * self.kappa)) for _ in self.output_shape\n    )\n    self.__setup_pipeline()\n</code></pre>"},{"location":"api/zarr_dataset/#cellulus.datasets.zarr_dataset.ZarrDataset.__yield_sample","title":"<code>__yield_sample()</code>","text":"<p>An infinite generator of crops.</p> Source code in <code>cellulus/datasets/zarr_dataset.py</code> <pre><code>def __yield_sample(self):\n    \"\"\"An infinite generator of crops.\"\"\"\n\n    with gp.build(self.pipeline):\n        while True:\n            array_is_zero = True\n            # request one sample, all channels, plus crop dimensions\n            while array_is_zero:\n                request = gp.BatchRequest()\n                request[self.raw] = gp.ArraySpec(\n                    roi=gp.Roi(\n                        (0,) * self.num_dims,\n                        (1, self.num_channels, *self.crop_size),\n                    )\n                )\n\n                sample = self.pipeline.request_batch(request)\n                sample_data = sample[self.raw].data[0]\n                if np.max(sample_data) &lt;= 0.0:\n                    pass\n                else:\n                    array_is_zero = False\n                    anchor_samples, reference_samples = self.sample_coordinates()\n            yield sample_data, anchor_samples, reference_samples\n</code></pre>"},{"location":"examples/2d/01-data/","title":"Download Data","text":"<p>In this notebook, we will download data and convert it to a zarr dataset.  This tutorial was written by Henry Westmacott and Manan Lalit.</p> <p>For demonstration, we will use a subset of images of <code>Fluo-N2DL-HeLa</code> available on the Cell Tracking Challenge webpage.</p> <p>Firstly, the <code>tif</code> raw images are downloaded to a directory indicated by <code>data_dir</code>.</p> In\u00a0[1]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[2]: Copied! <pre>import numpy as np\nimport tifffile\nimport zarr\nfrom cellulus.utils.misc import extract_data\nfrom csbdeep.utils import normalize\nfrom tqdm import tqdm\n</pre> import numpy as np import tifffile import zarr from cellulus.utils.misc import extract_data from csbdeep.utils import normalize from tqdm import tqdm In\u00a0[3]: Copied! <pre>name = \"2d-data-demo\"\ndata_dir = \"./data\"\n\nextract_data(\n    zip_url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-data-demo.zip\",\n    data_dir=data_dir,\n    project_name=name,\n)\n</pre> name = \"2d-data-demo\" data_dir = \"./data\"  extract_data(     zip_url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-data-demo.zip\",     data_dir=data_dir,     project_name=name, ) <pre>Created new directory ./data\n</pre> <pre>Downloaded and unzipped data to the location ./data\n</pre> <p>Next,  a channel dimension is added to these images and they are appended in a list.</p> In\u00a0[4]: Copied! <pre>container_path = zarr.open(name + \".zarr\")\ndataset_name = \"train/raw\"\nimage_filenames = sorted((Path(data_dir) / name / \"images\").glob(\"*.tif\"))\nprint(f\"Number of raw images is {len(image_filenames)}\")\nimage_list = []\n\nfor i in tqdm(range(len(image_filenames))):\n    im = normalize(\n        tifffile.imread(image_filenames[i]).astype(np.float32), 1, 99.8, axis=(0, 1)\n    )\n    image_list.append(im[np.newaxis, ...])\n\nimage_list = np.asarray(image_list)\n</pre> container_path = zarr.open(name + \".zarr\") dataset_name = \"train/raw\" image_filenames = sorted((Path(data_dir) / name / \"images\").glob(\"*.tif\")) print(f\"Number of raw images is {len(image_filenames)}\") image_list = []  for i in tqdm(range(len(image_filenames))):     im = normalize(         tifffile.imread(image_filenames[i]).astype(np.float32), 1, 99.8, axis=(0, 1)     )     image_list.append(im[np.newaxis, ...])  image_list = np.asarray(image_list) <pre>Number of raw images is 11\n</pre> <pre>\r  0%|                                                                                                                    | 0/11 [00:00&lt;?, ?it/s]</pre> <pre>\r 55%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589                                                 | 6/11 [00:00&lt;00:00, 55.28it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:00&lt;00:00, 57.79it/s]</pre> <pre>\n</pre> <p>Lastly, the zarr dataset is populated, the axis names and resolution is specified.</p> In\u00a0[5]: Copied! <pre>container_path[dataset_name] = image_list\ncontainer_path[dataset_name].attrs[\"resolution\"] = (1, 1)\ncontainer_path[dataset_name].attrs[\"axis_names\"] = (\"s\", \"c\", \"y\", \"x\")\n</pre> container_path[dataset_name] = image_list container_path[dataset_name].attrs[\"resolution\"] = (1, 1) container_path[dataset_name].attrs[\"axis_names\"] = (\"s\", \"c\", \"y\", \"x\")"},{"location":"examples/2d/01-data/#download-data","title":"Download Data\u00b6","text":""},{"location":"examples/2d/02-train/","title":"Train Model","text":"<p>In this notebook, we will train a <code>cellulus</code> model.</p> In\u00a0[1]: Copied! <pre>from attrs import asdict\nfrom cellulus.configs.dataset_config import DatasetConfig\nfrom cellulus.configs.experiment_config import ExperimentConfig\nfrom cellulus.configs.model_config import ModelConfig\nfrom cellulus.configs.train_config import TrainConfig\n</pre> from attrs import asdict from cellulus.configs.dataset_config import DatasetConfig from cellulus.configs.experiment_config import ExperimentConfig from cellulus.configs.model_config import ModelConfig from cellulus.configs.train_config import TrainConfig <p>In the next cell, we specify the name of the zarr container and the dataset within it from which data would be read.</p> In\u00a0[2]: Copied! <pre>name = \"2d-data-demo\"\ndataset_name = \"train/raw\"\n</pre> name = \"2d-data-demo\" dataset_name = \"train/raw\" In\u00a0[3]: Copied! <pre>train_data_config = DatasetConfig(\n    container_path=name + \".zarr\", dataset_name=dataset_name\n)\n</pre> train_data_config = DatasetConfig(     container_path=name + \".zarr\", dataset_name=dataset_name ) <p>In the next cell, we specify the number of feature maps (<code>num_fmaps</code>) in the first layer in our model.  Additionally, we specify <code>fmap_inc_factor</code>, which indicates by how much the number of feature maps increase between adjacent layers.</p> In\u00a0[4]: Copied! <pre>num_fmaps = 24\nfmap_inc_factor = 3\n</pre> num_fmaps = 24 fmap_inc_factor = 3 In\u00a0[5]: Copied! <pre>model_config = ModelConfig(num_fmaps=num_fmaps, fmap_inc_factor=fmap_inc_factor)\n</pre> model_config = ModelConfig(num_fmaps=num_fmaps, fmap_inc_factor=fmap_inc_factor) <p>Then, we specify training-specific parameters such as the <code>device</code>, which indicates the actual device to run the training on.  The device could be set equal to <code>cuda:n</code> (where <code>n</code> is the index of the GPU, for e.g. <code>cuda:0</code>), <code>cpu</code> or <code>mps</code>.  We set the <code>max_iterations</code> equal to 5000 for demonstration purposes. (This takes around 20 minutes on a Mac Book Pro with an Apple M2 Max chip).</p> In\u00a0[6]: Copied! <pre>device = \"cuda:0\"  # 'mps', 'cpu', 'cuda:0'\nmax_iterations = 5000\n</pre> device = \"cuda:0\"  # 'mps', 'cpu', 'cuda:0' max_iterations = 5000 In\u00a0[7]: Copied! <pre>train_config = TrainConfig(\n    train_data_config=asdict(train_data_config),\n    device=device,\n    max_iterations=max_iterations,\n)\n</pre> train_config = TrainConfig(     train_data_config=asdict(train_data_config),     device=device,     max_iterations=max_iterations, ) <p>Next, we initialize the experiment config which puts together the config objects (<code>train_config</code> and <code>model_config</code>) which we defined above.</p> In\u00a0[8]: Copied! <pre>experiment_config = ExperimentConfig(\n    train_config=asdict(train_config),\n    model_config=asdict(model_config),\n    normalization_factor=1.0,\n)\n</pre> experiment_config = ExperimentConfig(     train_config=asdict(train_config),     model_config=asdict(model_config),     normalization_factor=1.0, ) <p>Now we can begin the training!  Uncomment the next two lines to train the model.</p> In\u00a0[9]: Copied! <pre># from cellulus.train import train\n# train(experiment_config)\n</pre> # from cellulus.train import train # train(experiment_config)"},{"location":"examples/2d/02-train/#train-model","title":"Train Model\u00b6","text":""},{"location":"examples/2d/02-train/#specify-config-values-for-dataset","title":"Specify config values for dataset\u00b6","text":""},{"location":"examples/2d/02-train/#specify-config-values-for-model","title":"Specify config values for model\u00b6","text":""},{"location":"examples/2d/02-train/#specify-config-values-for-the-training-process","title":"Specify config values for the training process\u00b6","text":""},{"location":"examples/2d/03-infer/","title":"Infer using Trained Model","text":"<p>In this notebook, we will use the <code>cellulus</code> model trained in the previous step to obtain instance segmentations.</p> In\u00a0[1]: Copied! <pre>import urllib\nimport zipfile\n</pre> import urllib import zipfile In\u00a0[2]: Copied! <pre>import numpy as np\nimport skimage\nimport torch\nimport zarr\nfrom attrs import asdict\nfrom cellulus.configs.dataset_config import DatasetConfig\nfrom cellulus.configs.experiment_config import ExperimentConfig\nfrom cellulus.configs.inference_config import InferenceConfig\nfrom cellulus.configs.model_config import ModelConfig\nfrom cellulus.infer import infer\nfrom cellulus.utils.misc import visualize_2d\nfrom IPython.utils import io\nfrom matplotlib.colors import ListedColormap\n</pre> import numpy as np import skimage import torch import zarr from attrs import asdict from cellulus.configs.dataset_config import DatasetConfig from cellulus.configs.experiment_config import ExperimentConfig from cellulus.configs.inference_config import InferenceConfig from cellulus.configs.model_config import ModelConfig from cellulus.infer import infer from cellulus.utils.misc import visualize_2d from IPython.utils import io from matplotlib.colors import ListedColormap <p>We again specify <code>name</code> of the zarr container, and <code>dataset_name</code> which identifies the path to the raw image data, which needs to be segmented.</p> In\u00a0[3]: Copied! <pre>name = \"2d-data-demo\"\ndataset_name = \"train/raw\"\n</pre> name = \"2d-data-demo\" dataset_name = \"train/raw\" <p>We initialize the <code>dataset_config</code> which relates to the raw image data, <code>prediction_dataset_config</code> which relates to the per-pixel embeddings and the uncertainty, the <code>segmentation_dataset_config</code> which relates to the segmentations post the mean-shift clustering and the <code>post_processed_config</code> which relates to the segmentations after some post-processing.</p> In\u00a0[4]: Copied! <pre>dataset_config = DatasetConfig(container_path=name + \".zarr\", dataset_name=dataset_name)\nprediction_dataset_config = DatasetConfig(\n    container_path=name + \".zarr\", dataset_name=\"embeddings\"\n)\ndetection_dataset_config = DatasetConfig(\n    container_path=name + \".zarr\",\n    dataset_name=\"detection\",\n    secondary_dataset_name=\"embeddings\",\n)\nsegmentation_dataset_config = DatasetConfig(\n    container_path=name + \".zarr\",\n    dataset_name=\"segmentation\",\n    secondary_dataset_name=\"detection\",\n)\n</pre> dataset_config = DatasetConfig(container_path=name + \".zarr\", dataset_name=dataset_name) prediction_dataset_config = DatasetConfig(     container_path=name + \".zarr\", dataset_name=\"embeddings\" ) detection_dataset_config = DatasetConfig(     container_path=name + \".zarr\",     dataset_name=\"detection\",     secondary_dataset_name=\"embeddings\", ) segmentation_dataset_config = DatasetConfig(     container_path=name + \".zarr\",     dataset_name=\"segmentation\",     secondary_dataset_name=\"detection\", ) <p>We must also specify the <code>num_fmaps</code>, <code>fmap_inc_factor</code> (use same values as in the training step) and set <code>checkpoint</code> equal to <code>models/best_loss.pth</code> (best in terms of the lowest loss obtained).</p> <p>Here, we download a pretrained model trained by us for <code>5e3</code> iterations.  But please comment the next cell to use your own trained model, which should be available in the <code>models</code> directory.</p> In\u00a0[5]: Copied! <pre>torch.hub.download_url_to_file(\n    url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-demo-model.zip\",\n    dst=\"pretrained_model\",\n    progress=True,\n)\nwith zipfile.ZipFile(\"pretrained_model\", \"r\") as zip_ref:\n    zip_ref.extractall(\"\")\n</pre> torch.hub.download_url_to_file(     url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-demo-model.zip\",     dst=\"pretrained_model\",     progress=True, ) with zipfile.ZipFile(\"pretrained_model\", \"r\") as zip_ref:     zip_ref.extractall(\"\") <pre>\r  0%|                                                                                                               | 0.00/2.04M [00:00&lt;?, ?B/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.04M/2.04M [00:00&lt;00:00, 39.4MB/s]</pre> <pre>\n</pre> In\u00a0[6]: Copied! <pre>num_fmaps = 24\nfmap_inc_factor = 3\ncheckpoint = \"models/best_loss.pth\"\n</pre> num_fmaps = 24 fmap_inc_factor = 3 checkpoint = \"models/best_loss.pth\" In\u00a0[7]: Copied! <pre>model_config = ModelConfig(\n    num_fmaps=num_fmaps, fmap_inc_factor=fmap_inc_factor, checkpoint=checkpoint\n)\n</pre> model_config = ModelConfig(     num_fmaps=num_fmaps, fmap_inc_factor=fmap_inc_factor, checkpoint=checkpoint ) <p>Then, we specify inference-specific parameters such as the <code>device</code>, which indicates the actual device to run the inference on.  The device could be set equal to <code>cuda:n</code> (where <code>n</code> is the index of the GPU, for e.g. <code>cuda:0</code>), <code>cpu</code> or <code>mps</code>.</p> In\u00a0[8]: Copied! <pre>device = \"cuda:0\"  # \"cuda:0\", 'mps', 'cpu'\n</pre> device = \"cuda:0\"  # \"cuda:0\", 'mps', 'cpu' <p>We initialize the <code>inference_config</code> which contains our <code>embeddings_dataset_config</code>, <code>segmentation_dataset_config</code> and <code>post_processed_dataset_config</code>. We set post_processing to one of <code>cell</code> or <code>nucleus</code>, depending on if we would like the cell membrane to be segmented or the nucleus.</p> In\u00a0[9]: Copied! <pre>post_processing = \"nucleus\"\n</pre> post_processing = \"nucleus\" In\u00a0[10]: Copied! <pre>inference_config = InferenceConfig(\n    dataset_config=asdict(dataset_config),\n    prediction_dataset_config=asdict(prediction_dataset_config),\n    detection_dataset_config=asdict(detection_dataset_config),\n    segmentation_dataset_config=asdict(segmentation_dataset_config),\n    post_processing=post_processing,\n    device=device,\n)\n</pre> inference_config = InferenceConfig(     dataset_config=asdict(dataset_config),     prediction_dataset_config=asdict(prediction_dataset_config),     detection_dataset_config=asdict(detection_dataset_config),     segmentation_dataset_config=asdict(segmentation_dataset_config),     post_processing=post_processing,     device=device, ) <p>Lastly we initialize the <code>experiment_config</code> which contains the <code>inference_config</code> and <code>model_config</code> initialized above.</p> In\u00a0[11]: Copied! <pre>experiment_config = ExperimentConfig(\n    inference_config=asdict(inference_config),\n    model_config=asdict(model_config),\n    normalization_factor=1.0,  # since the data was already normalized.\n)\n</pre> experiment_config = ExperimentConfig(     inference_config=asdict(inference_config),     model_config=asdict(model_config),     normalization_factor=1.0,  # since the data was already normalized. ) <p>Now we are ready to start the inference!!  To see the output of the cell below, remove the first line <code>io.capture_output()</code>).</p> In\u00a0[12]: Copied! <pre>with io.capture_output() as captured:\n    infer(experiment_config)\n</pre> with io.capture_output() as captured:     infer(experiment_config) <p>Let's look at some of the predicted embeddings.  We will first load a glasbey-like color map to show individual cells with a unique color.</p> In\u00a0[13]: Copied! <pre>urllib.request.urlretrieve(\n    \"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/cmap_60.npy\",\n    \"cmap_60.npy\",\n)\nnew_cmp = ListedColormap(np.load(\"cmap_60.npy\"))\n</pre> urllib.request.urlretrieve(     \"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/cmap_60.npy\",     \"cmap_60.npy\", ) new_cmp = ListedColormap(np.load(\"cmap_60.npy\")) <p>Change the value of <code>index</code> below to look at the raw image (left), x-offset (bottom-left), y-offset (bottom-right) and uncertainty of the embedding (top-right).</p> In\u00a0[14]: Copied! <pre>index = 10\n\nf = zarr.open(name + \".zarr\")\nds = f[\"train/raw\"]\nds2 = f[\"centered-embeddings\"]\n\nimage = ds[index, 0]\nembedding = ds2[index]\n\nvisualize_2d(\n    image,\n    top_right=embedding[-1],\n    bottom_left=embedding[0],\n    bottom_right=embedding[1],\n    top_right_label=\"UNCERTAINTY\",\n    bottom_left_label=\"OFFSET_X\",\n    bottom_right_label=\"OFFSET_Y\",\n)\n</pre> index = 10  f = zarr.open(name + \".zarr\") ds = f[\"train/raw\"] ds2 = f[\"centered-embeddings\"]  image = ds[index, 0] embedding = ds2[index]  visualize_2d(     image,     top_right=embedding[-1],     bottom_left=embedding[0],     bottom_right=embedding[1],     top_right_label=\"UNCERTAINTY\",     bottom_left_label=\"OFFSET_X\",     bottom_right_label=\"OFFSET_Y\", ) <p>As you can see the magnitude of the uncertainty of the embedding (top-right) is low for most of the foreground cells.  This enables extraction of the foreground, which is eventually clustered into individual instances.</p> In\u00a0[15]: Copied! <pre>f = zarr.open(name + \".zarr\")\nds = f[\"train/raw\"]\nds2 = f[\"detection\"]\nds3 = f[\"segmentation\"]\n\nvisualize_2d(\n    image,\n    top_right=embedding[-1] &lt; skimage.filters.threshold_otsu(embedding[-1]),\n    bottom_left=ds2[index, 0],\n    bottom_right=ds3[index, 0],\n    top_right_label=\"THRESHOLDED F.G.\",\n    bottom_left_label=\"DETECTION\",\n    bottom_right_label=\"SEGMENTATION\",\n    top_right_cmap=\"gray\",\n    bottom_left_cmap=new_cmp,\n    bottom_right_cmap=new_cmp,\n)\n</pre> f = zarr.open(name + \".zarr\") ds = f[\"train/raw\"] ds2 = f[\"detection\"] ds3 = f[\"segmentation\"]  visualize_2d(     image,     top_right=embedding[-1] &lt; skimage.filters.threshold_otsu(embedding[-1]),     bottom_left=ds2[index, 0],     bottom_right=ds3[index, 0],     top_right_label=\"THRESHOLDED F.G.\",     bottom_left_label=\"DETECTION\",     bottom_right_label=\"SEGMENTATION\",     top_right_cmap=\"gray\",     bottom_left_cmap=new_cmp,     bottom_right_cmap=new_cmp, )"},{"location":"examples/2d/03-infer/#infer-using-trained-model","title":"Infer using Trained Model\u00b6","text":""},{"location":"examples/2d/03-infer/#specify-config-values-for-datasets","title":"Specify config values for datasets\u00b6","text":""},{"location":"examples/2d/03-infer/#specify-config-values-for-the-model","title":"Specify config values for the model\u00b6","text":""},{"location":"examples/2d/03-infer/#initialize-inference_config","title":"Initialize <code>inference_config</code>\u00b6","text":""},{"location":"examples/2d/03-infer/#initialize-experiment_config","title":"Initialize <code>experiment_config</code>\u00b6","text":""},{"location":"examples/2d/03-infer/#inspect-predictions","title":"Inspect predictions\u00b6","text":""},{"location":"examples/3d/01-data/","title":"Download Data","text":"<p>In this notebook, we will download data and convert it to a zarr dataset. </p> <p>For demonstration, we will use one image from the <code>Platynereis-Nuclei-CBG</code> dataset provided with this publication.</p> <p>Firstly, the <code>tif</code> raw images are downloaded to a directory indicated by <code>data_dir</code>.</p> In\u00a0[1]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[2]: Copied! <pre>import numpy as np\nimport tifffile\nimport zarr\nfrom cellulus.utils.misc import extract_data\nfrom csbdeep.utils import normalize\nfrom skimage.transform import rescale\nfrom tqdm import tqdm\n</pre> import numpy as np import tifffile import zarr from cellulus.utils.misc import extract_data from csbdeep.utils import normalize from skimage.transform import rescale from tqdm import tqdm In\u00a0[3]: Copied! <pre>name = \"3d-data-demo\"\ndata_dir = \"./data\"\n\nextract_data(\n    zip_url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/3d-data-demo.zip\",\n    data_dir=data_dir,\n    project_name=name,\n)\n</pre> name = \"3d-data-demo\" data_dir = \"./data\"  extract_data(     zip_url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/3d-data-demo.zip\",     data_dir=data_dir,     project_name=name, ) <pre>Downloaded and unzipped data to the location ./data\n</pre> <p>Currently, <code>cellulus</code> expects that the images are isotropic (i.e. the voxel size along z dimension (which is usually undersampled) is the same as the voxel size alng the x and y dimensions). This dataset has a step size of $2.031 \\mu m$ in z and $0.406 \\mu m$ along x and y dimensions, thus, the upsampling factor (which we refer to as <code>anisotropy</code> equals $2.031/0.406$.  These raw images are upsampled, intensity-normalized and appended in a list. Here, we use the percentile normalization technique.</p> In\u00a0[4]: Copied! <pre>anisotropy = 2.031 / 0.406\n</pre> anisotropy = 2.031 / 0.406 In\u00a0[5]: Copied! <pre>container_path = zarr.open(name + \".zarr\")\nsubsets = [\"train\"]\nfor subset in subsets:\n    dataset_name = subset + \"/raw\"\n    image_filenames = sorted((Path(data_dir) / name / subset).glob(\"*.tif\"))\n    print(f\"Number of raw images in {subset} directory is {len(image_filenames)}\")\n    image_list = []\n\n    for i in tqdm(range(len(image_filenames))):\n        im = tifffile.imread(image_filenames[i]).astype(np.float32)\n        im_normalized = normalize(im, 1, 99.8, axis=(0, 1, 2))\n        im_rescaled = rescale(im_normalized, (anisotropy, 1.0, 1.0))\n        image_list.append(im_rescaled[np.newaxis, ...])\n\n    image_list = np.asarray(image_list)\n    container_path[dataset_name] = image_list\n    container_path[dataset_name].attrs[\"resolution\"] = (1, 1, 1)\n    container_path[dataset_name].attrs[\"axis_names\"] = (\"s\", \"c\", \"z\", \"y\", \"x\")\n</pre> container_path = zarr.open(name + \".zarr\") subsets = [\"train\"] for subset in subsets:     dataset_name = subset + \"/raw\"     image_filenames = sorted((Path(data_dir) / name / subset).glob(\"*.tif\"))     print(f\"Number of raw images in {subset} directory is {len(image_filenames)}\")     image_list = []      for i in tqdm(range(len(image_filenames))):         im = tifffile.imread(image_filenames[i]).astype(np.float32)         im_normalized = normalize(im, 1, 99.8, axis=(0, 1, 2))         im_rescaled = rescale(im_normalized, (anisotropy, 1.0, 1.0))         image_list.append(im_rescaled[np.newaxis, ...])      image_list = np.asarray(image_list)     container_path[dataset_name] = image_list     container_path[dataset_name].attrs[\"resolution\"] = (1, 1, 1)     container_path[dataset_name].attrs[\"axis_names\"] = (\"s\", \"c\", \"z\", \"y\", \"x\") <pre>Number of raw images in train directory is 1\n</pre> <pre>\r  0%|                                                                                                                                                               | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:13&lt;00:00, 13.37s/it]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:13&lt;00:00, 13.38s/it]</pre> <pre>\n</pre>"},{"location":"examples/3d/01-data/#download-data","title":"Download Data\u00b6","text":""},{"location":"examples/3d/02-train/","title":"Train Model","text":"<p>In this notebook, we will train a <code>cellulus</code> model.</p> In\u00a0[1]: Copied! <pre>from attrs import asdict\nfrom cellulus.configs.dataset_config import DatasetConfig\nfrom cellulus.configs.experiment_config import ExperimentConfig\nfrom cellulus.configs.model_config import ModelConfig\nfrom cellulus.configs.train_config import TrainConfig\n</pre> from attrs import asdict from cellulus.configs.dataset_config import DatasetConfig from cellulus.configs.experiment_config import ExperimentConfig from cellulus.configs.model_config import ModelConfig from cellulus.configs.train_config import TrainConfig <p>In the next cell, we specify the name of the zarr container and the dataset within it from which data would be read.</p> In\u00a0[2]: Copied! <pre>name = \"3d-data-demo\"\ndataset_name = \"train/raw\"\n</pre> name = \"3d-data-demo\" dataset_name = \"train/raw\" In\u00a0[3]: Copied! <pre>train_data_config = DatasetConfig(\n    container_path=name + \".zarr\", dataset_name=dataset_name\n)\n</pre> train_data_config = DatasetConfig(     container_path=name + \".zarr\", dataset_name=dataset_name ) <p>In the next cell, we specify the number of feature maps (<code>num_fmaps</code>) in the first layer in our model.  Additionally, we specify <code>fmap_inc_factor</code> and <code>downsampling_factors</code>, which indicates by how much the number of feature maps increase between adjacent layers, and how much the spatial extents of the image gets downsampled between adjacent layers respectively.</p> In\u00a0[4]: Copied! <pre>num_fmaps = 24\nfmap_inc_factor = 3\ndownsampling_factors = [\n    [2, 2, 2],\n]\n</pre> num_fmaps = 24 fmap_inc_factor = 3 downsampling_factors = [     [2, 2, 2], ] In\u00a0[5]: Copied! <pre>model_config = ModelConfig(\n    num_fmaps=num_fmaps,\n    fmap_inc_factor=fmap_inc_factor,\n    downsampling_factors=downsampling_factors,\n)\n</pre> model_config = ModelConfig(     num_fmaps=num_fmaps,     fmap_inc_factor=fmap_inc_factor,     downsampling_factors=downsampling_factors, ) <p>Then, we specify training-specific parameters such as the <code>device</code>, which indicates the actual device to run the training on. We also specify the <code>crop_size</code>. Mini - batches of crops are shown to the model during training.  The device could be set equal to <code>cuda:n</code> (where <code>n</code> is the index of the GPU, for e.g. <code>cuda:0</code>) or <code>cpu</code>.  We set the <code>max_iterations</code> equal to <code>5000</code> for demonstration purposes.</p> In\u00a0[6]: Copied! <pre>device = \"cuda:0\"\nmax_iterations = 5000\ncrop_size = [80, 80, 80]\n</pre> device = \"cuda:0\" max_iterations = 5000 crop_size = [80, 80, 80] In\u00a0[7]: Copied! <pre>train_config = TrainConfig(\n    train_data_config=asdict(train_data_config),\n    device=device,\n    max_iterations=max_iterations,\n    crop_size=crop_size,\n)\n</pre> train_config = TrainConfig(     train_data_config=asdict(train_data_config),     device=device,     max_iterations=max_iterations,     crop_size=crop_size, ) <p>Next, we initialize the experiment config which puts together the config objects (<code>train_config</code> and <code>model_config</code>) which we defined above.</p> In\u00a0[8]: Copied! <pre>experiment_config = ExperimentConfig(\n    train_config=asdict(train_config),\n    model_config=asdict(model_config),\n    normalization_factor=1.0,  # since we already normalized in previous notebook\n)\n</pre> experiment_config = ExperimentConfig(     train_config=asdict(train_config),     model_config=asdict(model_config),     normalization_factor=1.0,  # since we already normalized in previous notebook ) <p>Now we can begin the training!  Uncomment the next two lines to train the model.</p> In\u00a0[9]: Copied! <pre># from cellulus.train import train\n# train(experiment_config)\n</pre> # from cellulus.train import train # train(experiment_config)"},{"location":"examples/3d/02-train/#train-model","title":"Train Model\u00b6","text":""},{"location":"examples/3d/02-train/#specify-config-values-for-dataset","title":"Specify config values for dataset\u00b6","text":""},{"location":"examples/3d/02-train/#specify-config-values-for-model","title":"Specify config values for model\u00b6","text":""},{"location":"examples/3d/02-train/#specify-config-values-for-the-training-process","title":"Specify config values for the training process\u00b6","text":""},{"location":"examples/3d/03-infer/","title":"Infer using Trained Model","text":"<p>In this notebook, we will use the <code>cellulus</code> model trained in the previous step to obtain instance segmentations.</p> In\u00a0[1]: Copied! <pre>import urllib\nimport zipfile\n</pre> import urllib import zipfile In\u00a0[2]: Copied! <pre>import numpy as np\nimport skimage\nimport torch\nimport zarr\nfrom attrs import asdict\nfrom cellulus.configs.dataset_config import DatasetConfig\nfrom cellulus.configs.experiment_config import ExperimentConfig\nfrom cellulus.configs.inference_config import InferenceConfig\nfrom cellulus.configs.model_config import ModelConfig\nfrom cellulus.infer import infer\nfrom cellulus.utils.misc import visualize_2d\nfrom matplotlib.colors import ListedColormap\n</pre> import numpy as np import skimage import torch import zarr from attrs import asdict from cellulus.configs.dataset_config import DatasetConfig from cellulus.configs.experiment_config import ExperimentConfig from cellulus.configs.inference_config import InferenceConfig from cellulus.configs.model_config import ModelConfig from cellulus.infer import infer from cellulus.utils.misc import visualize_2d from matplotlib.colors import ListedColormap <p>We again specify <code>name</code> of the zarr container, and <code>dataset_name</code> which identifies the path to the raw image data, which needs to be segmented.</p> In\u00a0[3]: Copied! <pre>name = \"3d-data-demo\"\ndataset_name = \"train/raw\"\n</pre> name = \"3d-data-demo\" dataset_name = \"train/raw\" <p>We initialize the <code>dataset_config</code> which relates to the raw image data, <code>prediction_dataset_config</code> which relates to the per-pixel embeddings and the uncertainty, the <code>segmentation_dataset_config</code> which relates to the segmentations post the mean-shift clustering and the <code>post_processed_config</code> which relates to the segmentations after some post-processing.</p> In\u00a0[4]: Copied! <pre>dataset_config = DatasetConfig(container_path=name + \".zarr\", dataset_name=dataset_name)\nprediction_dataset_config = DatasetConfig(\n    container_path=name + \".zarr\", dataset_name=\"embeddings\"\n)\ndetection_dataset_config = DatasetConfig(\n    container_path=name + \".zarr\",\n    dataset_name=\"detection\",\n    secondary_dataset_name=\"embeddings\",\n)\nsegmentation_dataset_config = DatasetConfig(\n    container_path=name + \".zarr\",\n    dataset_name=\"segmentation\",\n    secondary_dataset_name=\"detection\",\n)\n</pre> dataset_config = DatasetConfig(container_path=name + \".zarr\", dataset_name=dataset_name) prediction_dataset_config = DatasetConfig(     container_path=name + \".zarr\", dataset_name=\"embeddings\" ) detection_dataset_config = DatasetConfig(     container_path=name + \".zarr\",     dataset_name=\"detection\",     secondary_dataset_name=\"embeddings\", ) segmentation_dataset_config = DatasetConfig(     container_path=name + \".zarr\",     dataset_name=\"segmentation\",     secondary_dataset_name=\"detection\", ) <p>We must also specify the <code>num_fmaps</code>, <code>fmap_inc_factor</code> (use same values as in the training step) and set <code>checkpoint</code> equal to <code>models/best_loss.pth</code> (best in terms of the lowest loss obtained).</p> <p>Here, we download a pretrained model trained by us for <code>5e3</code> iterations.  But please comment the next cell to use your own trained model, which should be available in the <code>models</code> directory.</p> In\u00a0[5]: Copied! <pre>torch.hub.download_url_to_file(\n    url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-demo-model.zip\",\n    dst=\"pretrained_model\",\n    progress=True,\n)\nwith zipfile.ZipFile(\"pretrained_model\", \"r\") as zip_ref:\n    zip_ref.extractall(\"\")\n</pre> torch.hub.download_url_to_file(     url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-demo-model.zip\",     dst=\"pretrained_model\",     progress=True, ) with zipfile.ZipFile(\"pretrained_model\", \"r\") as zip_ref:     zip_ref.extractall(\"\") <pre>\r  0%|                                                                                                                                                         | 0.00/2.04M [00:00&lt;?, ?B/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.04M/2.04M [00:00&lt;00:00, 44.3MB/s]</pre> <pre>\n</pre> In\u00a0[6]: Copied! <pre>num_fmaps = 24\nfmap_inc_factor = 3\ndownsampling_factors = [\n    [2, 2, 2],\n]\ncheckpoint = \"models/best_loss.pth\"\n</pre> num_fmaps = 24 fmap_inc_factor = 3 downsampling_factors = [     [2, 2, 2], ] checkpoint = \"models/best_loss.pth\" In\u00a0[7]: Copied! <pre>model_config = ModelConfig(\n    num_fmaps=num_fmaps,\n    fmap_inc_factor=fmap_inc_factor,\n    downsampling_factors=downsampling_factors,\n    checkpoint=checkpoint,\n)\n</pre> model_config = ModelConfig(     num_fmaps=num_fmaps,     fmap_inc_factor=fmap_inc_factor,     downsampling_factors=downsampling_factors,     checkpoint=checkpoint, ) <p>Then, we specify inference-specific parameters such as the <code>device</code>, which indicates the actual device to run the inference on.  The device could be set equal to <code>cuda:n</code> (where <code>n</code> is the index of the GPU, for e.g. <code>cuda:0</code>), <code>cpu</code> or <code>mps</code>.</p> In\u00a0[8]: Copied! <pre>device = \"cuda:0\"\n</pre> device = \"cuda:0\" <p>We initialize the <code>inference_config</code> which contains our <code>embeddings_dataset_config</code>, <code>segmentation_dataset_config</code> and <code>post_processed_dataset_config</code>.</p> In\u00a0[9]: Copied! <pre>inference_config = InferenceConfig(\n    dataset_config=asdict(dataset_config),\n    prediction_dataset_config=asdict(prediction_dataset_config),\n    detection_dataset_config=asdict(detection_dataset_config),\n    segmentation_dataset_config=asdict(segmentation_dataset_config),\n    crop_size=[120, 120, 120],\n    post_processing=\"nucleus\",\n    device=device,\n    use_seeds=True,\n)\n</pre> inference_config = InferenceConfig(     dataset_config=asdict(dataset_config),     prediction_dataset_config=asdict(prediction_dataset_config),     detection_dataset_config=asdict(detection_dataset_config),     segmentation_dataset_config=asdict(segmentation_dataset_config),     crop_size=[120, 120, 120],     post_processing=\"nucleus\",     device=device,     use_seeds=True, ) <p>Lastly we initialize the <code>experiment_config</code> which contains the <code>inference_config</code> and <code>model_config</code> initialized above.</p> In\u00a0[10]: Copied! <pre>experiment_config = ExperimentConfig(\n    inference_config=asdict(inference_config),\n    model_config=asdict(model_config),\n    normalization_factor=1.0,  # since the image is already normalized\n)\n</pre> experiment_config = ExperimentConfig(     inference_config=asdict(inference_config),     model_config=asdict(model_config),     normalization_factor=1.0,  # since the image is already normalized ) <p>Now we are ready to start the inference!!  (To see the output of the cell below, remove the first line <code>io.capture_output()</code>).</p> In\u00a0[11]: Copied! <pre># with io.capture_output() as captured:\ninfer(experiment_config)\n</pre> # with io.capture_output() as captured: infer(experiment_config) <pre>ExperimentConfig(model_config=ModelConfig(num_fmaps=24, fmap_inc_factor=3, features_in_last_layer=64, downsampling_factors=[[2, 2, 2]], checkpoint=PosixPath('models/best_loss.pth'), initialize=True), experiment_name='2024-03-03', normalization_factor=1.0, object_size=30, train_config=None, inference_config=InferenceConfig(dataset_config=DatasetConfig(container_path=PosixPath('3d-data-demo.zarr'), dataset_name='train/raw', secondary_dataset_name=None), prediction_dataset_config=DatasetConfig(container_path=PosixPath('3d-data-demo.zarr'), dataset_name='embeddings', secondary_dataset_name=None), detection_dataset_config=DatasetConfig(container_path=PosixPath('3d-data-demo.zarr'), dataset_name='detection', secondary_dataset_name='embeddings'), segmentation_dataset_config=DatasetConfig(container_path=PosixPath('3d-data-demo.zarr'), dataset_name='segmentation', secondary_dataset_name='detection'), evaluation_dataset_config=None, device='cuda:0', crop_size=[120, 120, 120], p_salt_pepper=0.01, num_infer_iterations=16, threshold=None, clustering='meanshift', use_seeds=True, bandwidth=None, num_bandwidths=1, reduction_probability=0.1, min_size=None, post_processing='nucleus', grow_distance=3, shrink_distance=6))\n</pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[11], line 2\n      1 # with io.capture_output() as captured:\n----&gt; 2 infer(experiment_config)\n\nFile ~/github/cellulus/cellulus/infer.py:60, in infer(experiment_config)\n     58 if os.path.exists(model_config.checkpoint):\n     59     state = torch.load(model_config.checkpoint, map_location=device)\n---&gt; 60     model.load_state_dict(state[\"model_state_dict\"], strict=True)\n     61 else:\n     62     assert (\n     63         False\n     64     ), f\"Model weights do not exist at this location :{model_config.checkpoint}!\"\n\nFile /groups/funke/home/lalitm/miniconda3/envs/cellulus/lib/python3.9/site-packages/torch/nn/modules/module.py:2041, in Module.load_state_dict(self, state_dict, strict)\n   2036         error_msgs.insert(\n   2037             0, 'Missing key(s) in state_dict: {}. '.format(\n   2038                 ', '.join('\"{}\"'.format(k) for k in missing_keys)))\n   2040 if len(error_msgs) &gt; 0:\n-&gt; 2041     raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n   2042                        self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n   2043 return _IncompatibleKeys(missing_keys, unexpected_keys)\n\nRuntimeError: Error(s) in loading state_dict for UNetModel:\n\tsize mismatch for backbone.l_conv.0.conv_pass.0.weight: copying a param with shape torch.Size([24, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 1, 3, 3, 3]).\n\tsize mismatch for backbone.l_conv.0.conv_pass.2.weight: copying a param with shape torch.Size([24, 24, 1, 1]) from checkpoint, the shape in current model is torch.Size([24, 24, 1, 1, 1]).\n\tsize mismatch for backbone.l_conv.0.conv_pass.4.weight: copying a param with shape torch.Size([24, 24, 1, 1]) from checkpoint, the shape in current model is torch.Size([24, 24, 1, 1, 1]).\n\tsize mismatch for backbone.l_conv.0.conv_pass.6.weight: copying a param with shape torch.Size([24, 24, 3, 3]) from checkpoint, the shape in current model is torch.Size([24, 24, 3, 3, 3]).\n\tsize mismatch for backbone.l_conv.1.conv_pass.0.weight: copying a param with shape torch.Size([72, 24, 3, 3]) from checkpoint, the shape in current model is torch.Size([72, 24, 3, 3, 3]).\n\tsize mismatch for backbone.l_conv.1.conv_pass.2.weight: copying a param with shape torch.Size([72, 72, 1, 1]) from checkpoint, the shape in current model is torch.Size([72, 72, 1, 1, 1]).\n\tsize mismatch for backbone.l_conv.1.conv_pass.4.weight: copying a param with shape torch.Size([72, 72, 1, 1]) from checkpoint, the shape in current model is torch.Size([72, 72, 1, 1, 1]).\n\tsize mismatch for backbone.l_conv.1.conv_pass.6.weight: copying a param with shape torch.Size([72, 72, 3, 3]) from checkpoint, the shape in current model is torch.Size([72, 72, 3, 3, 3]).\n\tsize mismatch for backbone.r_conv.0.0.conv_pass.0.weight: copying a param with shape torch.Size([64, 96, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 96, 3, 3, 3]).\n\tsize mismatch for backbone.r_conv.0.0.conv_pass.2.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1, 1]).\n\tsize mismatch for backbone.r_conv.0.0.conv_pass.4.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1, 1]).\n\tsize mismatch for backbone.r_conv.0.0.conv_pass.6.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3, 3]).\n\tsize mismatch for head.0.weight: copying a param with shape torch.Size([64, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([64, 64, 1, 1, 1]).\n\tsize mismatch for head.2.weight: copying a param with shape torch.Size([2, 64, 1, 1]) from checkpoint, the shape in current model is torch.Size([3, 64, 1, 1, 1]).\n\tsize mismatch for head.2.bias: copying a param with shape torch.Size([2]) from checkpoint, the shape in current model is torch.Size([3]).</pre> <p>Let's look at some of the predicted embeddings.  We will first load a glasbey-like color map to show individual cells with a unique color.</p> In\u00a0[12]: Copied! <pre>urllib.request.urlretrieve(\n    \"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/cmap_60.npy\",\n    \"cmap_60.npy\",\n)\nnew_cmp = ListedColormap(np.load(\"cmap_60.npy\"))\n</pre> urllib.request.urlretrieve(     \"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/cmap_60.npy\",     \"cmap_60.npy\", ) new_cmp = ListedColormap(np.load(\"cmap_60.npy\")) <p>Change the value of <code>index</code> below to look at the raw image (left), x-offset (bottom-left), y-offset (bottom-right) and uncertainty of the embedding (top-right).</p> In\u00a0[13]: Copied! <pre>index = 0\n\nf = zarr.open(name + \".zarr\")\nds = f[\"train/raw\"]\nds2 = f[\"centered-embeddings\"]\n\nslice = ds.shape[2] // 2\n\nimage = ds[index, 0, slice]\nembedding = ds2[index, :, slice]\n\n\nvisualize_2d(\n    image,\n    top_right=embedding[-1],\n    bottom_left=embedding[0],\n    bottom_right=embedding[1],\n    top_right_label=\"UNCERTAINTY\",\n    bottom_left_label=\"OFFSET_X\",\n    bottom_right_label=\"OFFSET_Y\",\n)\n</pre> index = 0  f = zarr.open(name + \".zarr\") ds = f[\"train/raw\"] ds2 = f[\"centered-embeddings\"]  slice = ds.shape[2] // 2  image = ds[index, 0, slice] embedding = ds2[index, :, slice]   visualize_2d(     image,     top_right=embedding[-1],     bottom_left=embedding[0],     bottom_right=embedding[1],     top_right_label=\"UNCERTAINTY\",     bottom_left_label=\"OFFSET_X\",     bottom_right_label=\"OFFSET_Y\", ) <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[13], line 5\n      3 f = zarr.open(name + \".zarr\")\n      4 ds = f[\"train/raw\"]\n----&gt; 5 ds2 = f[\"centered-embeddings\"]\n      7 slice = ds.shape[2] // 2\n      9 image = ds[index, 0, slice]\n\nFile /groups/funke/home/lalitm/miniconda3/envs/cellulus/lib/python3.9/site-packages/zarr/hierarchy.py:500, in Group.__getitem__(self, item)\n    498         raise KeyError(item)\n    499 else:\n--&gt; 500     raise KeyError(item)\n\nKeyError: 'centered-embeddings'</pre> <p>As you can see the magnitude of the uncertainty of the embedding (top-right) is low for most of the foreground cells.  This enables extraction of the foreground, which is eventually clustered into individual instances.  See bottom right figure for the final result.</p> In\u00a0[14]: Copied! <pre>f = zarr.open(name + \".zarr\")\nds = f[\"train/raw\"]\nds2 = f[\"detection\"]\nds3 = f[\"segmentation\"]\n\nvisualize_2d(\n    image,\n    top_right=embedding[-1] &lt; skimage.filters.threshold_otsu(embedding[-1]),\n    bottom_left=ds2[index, index, slice],\n    bottom_right=ds3[index, index, slice],\n    top_right_label=\"THRESHOLDED F.G.\",\n    bottom_left_label=\"DETECTION\",\n    bottom_right_label=\"SEGMENTATION\",\n    top_right_cmap=\"gray\",\n    bottom_left_cmap=new_cmp,\n    bottom_right_cmap=new_cmp,\n)\n</pre> f = zarr.open(name + \".zarr\") ds = f[\"train/raw\"] ds2 = f[\"detection\"] ds3 = f[\"segmentation\"]  visualize_2d(     image,     top_right=embedding[-1] &lt; skimage.filters.threshold_otsu(embedding[-1]),     bottom_left=ds2[index, index, slice],     bottom_right=ds3[index, index, slice],     top_right_label=\"THRESHOLDED F.G.\",     bottom_left_label=\"DETECTION\",     bottom_right_label=\"SEGMENTATION\",     top_right_cmap=\"gray\",     bottom_left_cmap=new_cmp,     bottom_right_cmap=new_cmp, ) <pre>\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[14], line 3\n      1 f = zarr.open(name + \".zarr\")\n      2 ds = f[\"train/raw\"]\n----&gt; 3 ds2 = f[\"detection\"]\n      4 ds3 = f[\"segmentation\"]\n      6 visualize_2d(\n      7     image,\n      8     top_right=embedding[-1] &lt; skimage.filters.threshold_otsu(embedding[-1]),\n   (...)\n     16     bottom_right_cmap=new_cmp,\n     17 )\n\nFile /groups/funke/home/lalitm/miniconda3/envs/cellulus/lib/python3.9/site-packages/zarr/hierarchy.py:500, in Group.__getitem__(self, item)\n    498         raise KeyError(item)\n    499 else:\n--&gt; 500     raise KeyError(item)\n\nKeyError: 'detection'</pre>"},{"location":"examples/3d/03-infer/#infer-using-trained-model","title":"Infer using Trained Model\u00b6","text":""},{"location":"examples/3d/03-infer/#specify-config-values-for-datasets","title":"Specify config values for datasets\u00b6","text":""},{"location":"examples/3d/03-infer/#specify-config-values-for-the-model","title":"Specify config values for the model\u00b6","text":""},{"location":"examples/3d/03-infer/#initialize-inference_config","title":"Initialize <code>inference_config</code>\u00b6","text":""},{"location":"examples/3d/03-infer/#initialize-experiment_config","title":"Initialize <code>experiment_config</code>\u00b6","text":""},{"location":"examples/3d/03-infer/#inspect-predictions","title":"Inspect predictions\u00b6","text":""}]}