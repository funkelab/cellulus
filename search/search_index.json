{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"getting-started/","title":"Installation","text":""},{"location":"getting-started/#using-the-terminal","title":"Using the terminal","text":"<p>If you would like to run <code>cellulus</code> , using GPU:</p> <pre><code>conda create -y -n cellulus python==3.9\nconda activate cellulus\nconda install pytorch==2.0.1 torchvision==0.15.2 pytorch-cuda=11.7 -c pytorch -c nvidia\ngit clone https://github.com/funkelab/cellulus.git\ncd cellulus\npip install -e .\n</code></pre> <p>If you would like to run <code>cellulus</code>, using the CPU or the MPS framework:</p> <pre><code>conda create -y -n cellulus python==3.9\nconda activate cellulus\npip install torch torchvision\ngit clone https://github.com/funkelab/cellulus.git\ncd cellulus\npip install -e .\n</code></pre>"},{"location":"getting-started/#using-the-napari-plugin","title":"Using the <code>napari</code> plugin","text":""},{"location":"api/dataset_config/","title":"DatasetConfig","text":"<p>Dataset configuration.</p> <p>Parameters:</p> <pre><code>container_path:\n\n    A path to the zarr/N5 container.\n\ndataset_name:\n\n    The name of the dataset containing raw data in the container.\n\nsecondary_dataset_name:\n\n    The name of the dataset containing the data which needs processing.\n</code></pre> Source code in <code>cellulus/configs/dataset_config.py</code> <pre><code>@attrs.define\nclass DatasetConfig:\n    \"\"\"Dataset configuration.\n\n    Parameters:\n\n        container_path:\n\n            A path to the zarr/N5 container.\n\n        dataset_name:\n\n            The name of the dataset containing raw data in the container.\n\n        secondary_dataset_name:\n\n            The name of the dataset containing the data which needs processing.\n\n\n    \"\"\"\n\n    container_path: Path = attrs.field(converter=Path)\n    dataset_name: str = attrs.field(validator=instance_of(str))\n    secondary_dataset_name: str = attrs.field(\n        default=None, validator=optional(instance_of(str))\n    )\n</code></pre>"},{"location":"api/experiment_config/","title":"ExperimentConfig","text":"<p>Top-level config for an experiment (containing training and prediction).</p> <p>Parameters:</p> <pre><code>experiment_name: (default = 'YYYY-MM-DD')\n\n    A unique name for the experiment.\n\nobject_size: (default = 26.0)\n\n    A rough estimate of the size of objects in the image, given in\n    world units. The \"patch size\" of the network will be chosen based\n    on this estimate.\n\nmodel_config:\n\n    The model configuration.\n\ntrain_config:\n\n    Configuration object for training.\n\ninference_config:\n\n    Configuration object for prediction.\n</code></pre> Source code in <code>cellulus/configs/experiment_config.py</code> <pre><code>@attrs.define\nclass ExperimentConfig:\n    \"\"\"Top-level config for an experiment (containing training and prediction).\n\n    Parameters:\n\n        experiment_name: (default = 'YYYY-MM-DD')\n\n            A unique name for the experiment.\n\n        object_size: (default = 26.0)\n\n            A rough estimate of the size of objects in the image, given in\n            world units. The \"patch size\" of the network will be chosen based\n            on this estimate.\n\n        model_config:\n\n            The model configuration.\n\n        train_config:\n\n            Configuration object for training.\n\n        inference_config:\n\n            Configuration object for prediction.\n    \"\"\"\n\n    model_config: ModelConfig = attrs.field(converter=to_config(ModelConfig))\n    experiment_name: str = attrs.field(\n        default=datetime.today().strftime(\"%Y-%m-%d\"), validator=instance_of(str)\n    )\n    object_size: float = attrs.field(default=26.0, validator=instance_of(float))\n\n    train_config: TrainConfig = attrs.field(\n        default=None, converter=to_config(TrainConfig)\n    )\n    inference_config: InferenceConfig = attrs.field(\n        default=None, converter=to_config(InferenceConfig)\n    )\n</code></pre>"},{"location":"api/inference_config/","title":"InferenceConfig","text":"<p>Inference configuration.</p> <p>Parameters:</p> <pre><code>dataset_config:\n\n    Configuration object for the data to predict and segment.\n\nprediction_dataset_config:\n\n    Configuration object produced by predict.py.\n\nsegmentation_dataset_config:\n\n    Configuration object produced by segment.py.\n\npost_processed_dataset_config:\n\n    Configuration object produced by postprocess.py.\n\nevaluation_dataset_config:\n\n    Configuration object for the ground truth masks.\n\ncrop_size:\n\n    ROI used by the scan node in gunpowder.\n\np_salt_pepper:\n\n    Fraction of pixels that will have salt-pepper noise.\n\nnum_infer_iterations:\n\n    Number of times the salt-peper noise is added to the raw image.\n\nbandwidth:\n\n    Band-width used to perform mean-shift clustering on the predicted\n    embeddings.\n\nthreshold:\n\n    Threshold to use for binary partitioning into foreground and background\n    pixel regions. If None, this is figured out automatically by performing\n    Otsu Thresholding on the last channel of the predicted embeddings.\n\nreduction_probability:\n\n\nmin_size:\n\n    Ignore objects which are smaller than min_size number of pixels.\n\ndevice (default = 'cuda:0'):\n\n    The device to infer on.\n    Set to 'cpu' to infer without GPU.\n\nnum_bandwidths (default = 1):\n\n    Number of bandwidths to obtain segmentations for.\n</code></pre> Source code in <code>cellulus/configs/inference_config.py</code> <pre><code>@attrs.define\nclass InferenceConfig:\n    \"\"\"Inference configuration.\n\n    Parameters:\n\n        dataset_config:\n\n            Configuration object for the data to predict and segment.\n\n        prediction_dataset_config:\n\n            Configuration object produced by predict.py.\n\n        segmentation_dataset_config:\n\n            Configuration object produced by segment.py.\n\n        post_processed_dataset_config:\n\n            Configuration object produced by postprocess.py.\n\n        evaluation_dataset_config:\n\n            Configuration object for the ground truth masks.\n\n        crop_size:\n\n            ROI used by the scan node in gunpowder.\n\n        p_salt_pepper:\n\n            Fraction of pixels that will have salt-pepper noise.\n\n        num_infer_iterations:\n\n            Number of times the salt-peper noise is added to the raw image.\n\n        bandwidth:\n\n            Band-width used to perform mean-shift clustering on the predicted\n            embeddings.\n\n        threshold:\n\n            Threshold to use for binary partitioning into foreground and background\n            pixel regions. If None, this is figured out automatically by performing\n            Otsu Thresholding on the last channel of the predicted embeddings.\n\n        reduction_probability:\n\n\n        min_size:\n\n            Ignore objects which are smaller than min_size number of pixels.\n\n        device (default = 'cuda:0'):\n\n            The device to infer on.\n            Set to 'cpu' to infer without GPU.\n\n        num_bandwidths (default = 1):\n\n            Number of bandwidths to obtain segmentations for.\n\n    \"\"\"\n\n    dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n\n    prediction_dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n\n    segmentation_dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n\n    post_processed_dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n\n    evaluation_dataset_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n    device: str = attrs.field(default=\"cuda:0\", validator=instance_of(str))\n    crop_size: List = attrs.field(default=[252, 252], validator=instance_of(List))\n    p_salt_pepper = attrs.field(default=0.01, validator=instance_of(float))\n    num_infer_iterations = attrs.field(default=16, validator=instance_of(int))\n    threshold = attrs.field(\n        default=None, validator=attrs.validators.optional(instance_of(float))\n    )\n    bandwidth = attrs.field(\n        default=None, validator=attrs.validators.optional(instance_of(int))\n    )\n    num_bandwidths = attrs.field(default=1, validator=instance_of(int))\n    reduction_probability = attrs.field(default=0.1, validator=instance_of(float))\n    min_size = attrs.field(\n        default=None, validator=attrs.validators.optional(instance_of(int))\n    )\n    grow_distance = attrs.field(default=3, validator=instance_of(int))\n    shrink_distance = attrs.field(default=6, validator=instance_of(int))\n</code></pre>"},{"location":"api/model_config/","title":"ModelConfig","text":"<p>Model configuration.</p> <p>Parameters:</p> <pre><code>num_fmaps:\n\n    The number of feature maps in the first level of the U-Net.\n\nfmap_inc_factor:\n\n    The factor by which to increase the number of feature maps between\n    levels of the U-Net.\n\nfeatures_in_last_layer (optional, default = 64):\n\n    The number of feature channels in the last layer of the U-Net\n\ndownsampling_factors:\n\n    A list of downsampling factors, each given per dimension (e.g.,\n    [[2,2], [3,3]] would correspond to two downsample layers, one with\n    an isotropic factor of 2, and another one with 3). This parameter\n    will also determine the number of levels in the U-Net.\n\ncheckpoint (optional, default ``None``):\n\n    A path to a checkpoint of the network. Needs to be set for networks\n    that are used for prediction. If set during training, the\n    checkpoint will be used to resume training, otherwise the network\n    will be trained from scratch.\n\ninitialize (default: True)\n\n    If True, initialize the model weights with Kaiming Normal\n</code></pre> Source code in <code>cellulus/configs/model_config.py</code> <pre><code>@attrs.define\nclass ModelConfig:\n    \"\"\"Model configuration.\n\n    Parameters:\n\n        num_fmaps:\n\n            The number of feature maps in the first level of the U-Net.\n\n        fmap_inc_factor:\n\n            The factor by which to increase the number of feature maps between\n            levels of the U-Net.\n\n        features_in_last_layer (optional, default = 64):\n\n            The number of feature channels in the last layer of the U-Net\n\n        downsampling_factors:\n\n            A list of downsampling factors, each given per dimension (e.g.,\n            [[2,2], [3,3]] would correspond to two downsample layers, one with\n            an isotropic factor of 2, and another one with 3). This parameter\n            will also determine the number of levels in the U-Net.\n\n        checkpoint (optional, default ``None``):\n\n            A path to a checkpoint of the network. Needs to be set for networks\n            that are used for prediction. If set during training, the\n            checkpoint will be used to resume training, otherwise the network\n            will be trained from scratch.\n\n        initialize (default: True)\n\n            If True, initialize the model weights with Kaiming Normal\n\n    \"\"\"\n\n    num_fmaps: int = attrs.field(validator=instance_of(int))\n    fmap_inc_factor: int = attrs.field(validator=instance_of(int))\n    features_in_last_layer: int = attrs.field(default=64)\n    downsampling_factors: List[List[int]] = attrs.field(\n        default=[\n            [2, 2],\n        ]\n    )\n    checkpoint: Path = attrs.field(default=None, converter=to_path)\n    initialize: bool = attrs.field(default=True, validator=instance_of(bool))\n</code></pre>"},{"location":"api/oce_loss/","title":"OCELoss","text":"<p>             Bases: <code>Module</code></p> Source code in <code>cellulus/criterions/oce_loss.py</code> <pre><code>class OCELoss(nn.Module):  # type: ignore\n    def __init__(\n        self,\n        temperature: float,\n        regularization_weight: float,\n        density: float,\n        kappa: float,\n        num_spatial_dims: int,\n        reduce_mean: bool,\n        device: torch.device,\n    ):\n        \"\"\"Class definition for loss.\n\n        Parameters\n        ----------\n\n            temperature:\n                Factor used to scale the gaussian function and control\n                the rate of damping.\n\n            regularization_weight:\n                The weight of the L2 regularizer on the object-centric embeddings.\n\n            density:\n                Determines the fraction of patches to sample per crop,\n                during training.\n\n            kappa:\n                Neighborhood radius to extract patches from.\n\n            num_spatial_dims:\n                Should be equal to 2 for 2D and 3 for 3D.\n\n            reduce_mean:\n                Should be set to True if the loss should be averaged over all\n                pixels, and set to False, if the sum of the loss over all pixels\n                is expected.\n\n            device:\n                The device to train on.\n                Set to 'cpu' to train without GPU.\n\n        \"\"\"\n        super().__init__()\n        self.temperature = temperature\n        self.regularization_weight = regularization_weight\n        self.density = density\n        self.kappa = kappa\n        self.num_spatial_dims = num_spatial_dims\n        self.reduce_mean = reduce_mean\n        self.device = device\n\n    def distance_function(self, e0, e1):\n        diff = e0 - e1\n        return diff.norm(2, dim=-1)\n\n    def nonlinearity(self, distance):\n        return 1 - (-distance.pow(2) / self.temperature).exp()\n\n    def forward(self, prediction):\n        if self.num_spatial_dims == 2:\n            b, c, h, w = prediction.shape\n\n            h_ = h - 2 * self.kappa  # unbiased height\n            w_ = w - 2 * self.kappa  # unbiased width\n\n            num_anchors = int(self.density * h_ * w_)\n            anchor_coordinates_y = np.random.randint(\n                self.kappa, h - self.kappa, num_anchors\n            )\n            anchor_coordinates_x = np.random.randint(\n                self.kappa, w - self.kappa, num_anchors\n            )\n            anchor_coordinates = np.stack(\n                (anchor_coordinates_x, anchor_coordinates_y), axis=1\n            )  # N x 2\n        elif self.num_spatial_dims == 3:\n            b, c, d, h, w = prediction.shape\n\n            d_ = d - 2 * self.kappa  # unbiased depth\n            h_ = h - 2 * self.kappa  # unbiased height\n            w_ = w - 2 * self.kappa  # unbiased width\n\n            num_anchors = int(self.density * d_ * h_ * w_)\n            anchor_coordinates_z = np.random.randint(\n                self.kappa, d - self.kappa, num_anchors\n            )\n            anchor_coordinates_y = np.random.randint(\n                self.kappa, h - self.kappa, num_anchors\n            )\n            anchor_coordinates_x = np.random.randint(\n                self.kappa, w - self.kappa, num_anchors\n            )\n            anchor_coordinates = np.stack(\n                (anchor_coordinates_x, anchor_coordinates_y, anchor_coordinates_z),\n                axis=1,\n            )  # N x 3\n        num_references = int(self.density * np.pi * self.kappa**2)\n        anchor_coordinates = np.repeat(anchor_coordinates, num_references, axis=0)\n        offsets = self.sample_offsets(\n            radius=self.kappa,\n            num_samples=len(anchor_coordinates),\n        )\n        reference_coordinates = anchor_coordinates + offsets\n        anchor_coordinates = anchor_coordinates[np.newaxis, ...]\n        reference_coordinates = reference_coordinates[np.newaxis, ...]\n        anchor_coordinates = torch.from_numpy(np.repeat(anchor_coordinates, b, 0)).to(\n            self.device\n        )\n        reference_coordinates = torch.from_numpy(\n            np.repeat(reference_coordinates, b, 0)\n        ).to(self.device)\n        anchor_embeddings = self.get_embeddings(\n            prediction,\n            anchor_coordinates,\n        )  # B x N x 2/3\n        reference_embeddings = self.get_embeddings(\n            prediction,\n            reference_coordinates,\n        )  # B x N x 2/3\n        distance = self.distance_function(\n            anchor_embeddings, reference_embeddings.detach()\n        )\n        oce_loss = self.nonlinearity(distance)\n        regularization_loss = self.regularization_weight * anchor_embeddings.norm(\n            2, dim=-1\n        )\n\n        loss = oce_loss + regularization_loss\n        if self.reduce_mean:\n            return loss.mean(), oce_loss.mean(), regularization_loss.mean()\n        else:\n            return loss.sum(), oce_loss.sum(), regularization_loss.sum()\n\n    def sample_offsets(self, radius, num_samples):\n        if self.num_spatial_dims == 2:\n            offset_x = np.random.randint(-radius, radius + 1, size=2 * num_samples)\n            offset_y = np.random.randint(-radius, radius + 1, size=2 * num_samples)\n\n            offset_coordinates = np.stack((offset_x, offset_y), axis=1)\n        elif self.num_spatial_dims == 3:\n            offset_x = np.random.randint(-radius, radius + 1, size=3 * num_samples)\n            offset_y = np.random.randint(-radius, radius + 1, size=3 * num_samples)\n            offset_z = np.random.randint(-radius, radius + 1, size=3 * num_samples)\n\n            offset_coordinates = np.stack((offset_x, offset_y, offset_z), axis=1)\n        in_circle = (offset_coordinates**2).sum(axis=1) &lt; radius**2\n        offset_coordinates = offset_coordinates[in_circle]\n        not_zero = np.absolute(offset_coordinates).sum(axis=1) &gt; 0\n        offset_coordinates = offset_coordinates[not_zero]\n        if len(offset_coordinates) &lt; num_samples:\n            return self.sample_offsets(radius, num_samples)\n\n        return offset_coordinates[:num_samples]\n\n    def get_embeddings(self, predictions, coordinates):\n        selection = []\n        for prediction, coordinate in zip(predictions, coordinates):\n            if self.num_spatial_dims == 2:\n                embedding = prediction[\n                    :, coordinate[:, 1].long(), coordinate[:, 0].long()\n                ]\n            elif self.num_spatial_dims == 3:\n                embedding = prediction[\n                    :,\n                    coordinate[:, 2].long(),\n                    coordinate[:, 1].long(),\n                    coordinate[:, 0].long(),\n                ]\n            embedding = embedding.transpose(1, 0)\n            embedding += coordinate\n            selection.append(embedding)\n\n        # selection.shape = (b, c, p) where p is the number of selected positions\n        return torch.stack(selection, dim=0)\n</code></pre>"},{"location":"api/oce_loss/#cellulus.criterions.oce_loss.OCELoss.__init__","title":"<code>__init__(temperature, regularization_weight, density, kappa, num_spatial_dims, reduce_mean, device)</code>","text":"<p>Class definition for loss.</p>"},{"location":"api/oce_loss/#cellulus.criterions.oce_loss.OCELoss.__init__--parameters","title":"Parameters","text":"<pre><code>temperature:\n    Factor used to scale the gaussian function and control\n    the rate of damping.\n\nregularization_weight:\n    The weight of the L2 regularizer on the object-centric embeddings.\n\ndensity:\n    Determines the fraction of patches to sample per crop,\n    during training.\n\nkappa:\n    Neighborhood radius to extract patches from.\n\nnum_spatial_dims:\n    Should be equal to 2 for 2D and 3 for 3D.\n\nreduce_mean:\n    Should be set to True if the loss should be averaged over all\n    pixels, and set to False, if the sum of the loss over all pixels\n    is expected.\n\ndevice:\n    The device to train on.\n    Set to 'cpu' to train without GPU.\n</code></pre> Source code in <code>cellulus/criterions/oce_loss.py</code> <pre><code>def __init__(\n    self,\n    temperature: float,\n    regularization_weight: float,\n    density: float,\n    kappa: float,\n    num_spatial_dims: int,\n    reduce_mean: bool,\n    device: torch.device,\n):\n    \"\"\"Class definition for loss.\n\n    Parameters\n    ----------\n\n        temperature:\n            Factor used to scale the gaussian function and control\n            the rate of damping.\n\n        regularization_weight:\n            The weight of the L2 regularizer on the object-centric embeddings.\n\n        density:\n            Determines the fraction of patches to sample per crop,\n            during training.\n\n        kappa:\n            Neighborhood radius to extract patches from.\n\n        num_spatial_dims:\n            Should be equal to 2 for 2D and 3 for 3D.\n\n        reduce_mean:\n            Should be set to True if the loss should be averaged over all\n            pixels, and set to False, if the sum of the loss over all pixels\n            is expected.\n\n        device:\n            The device to train on.\n            Set to 'cpu' to train without GPU.\n\n    \"\"\"\n    super().__init__()\n    self.temperature = temperature\n    self.regularization_weight = regularization_weight\n    self.density = density\n    self.kappa = kappa\n    self.num_spatial_dims = num_spatial_dims\n    self.reduce_mean = reduce_mean\n    self.device = device\n</code></pre>"},{"location":"api/train_config/","title":"TrainConfig","text":"<p>Train configuration.</p> <p>Parameters:</p> <pre><code>crop_size:\n\n    The size of the crops - specified as a list of number of pixels -\n    extracted from the raw images, used during training.\n\nbatch_size:\n\n    The number of samples to use per batch.\n\nmax_iterations:\n\n    The maximum number of iterations to train for.\n\ninitial_learning_rate (default = 4e-5):\n\n    Initial learning rate of the optimizer.\n\ntemperature (default = 10):\n\n    Factor used to scale the gaussian function and control the rate of damping.\n\nregularizer_weight (default = 1e-5):\n\n    The weight of the L2 regularizer on the object-centric embeddings.\n\nreduce_mean (default = True):\n\n    If True, the loss contribution is averaged across all pairs of patches.\n\ndensity (default = 0.1)\n\n    Determines the fraction of patches to sample per crop, during training.\n\nkappa (default = 10.0):\n\n    Neighborhood radius to extract patches from.\n\nsave_model_every (default = 1e3):\n\n    The model weights are saved every few iterations.\n\nsave_snapshot_every (default = 1e3):\n\n    The zarr snapshot is saved every few iterations.\n\nnum_workers (default = 8):\n\n    The number of sub-processes to use for data-loading.\n\ncontrol_point_spacing (default = 64):\n\n    The distance in pixels between control points used for elastic\n    deformation of the raw data during training.\n\ncontrol_point_jitter (default = 2.0):\n\n    How much to jitter the control points for elastic deformation\n    of the raw data during training, given as the standard deviation of\n    a normal distribution with zero mean.\n\ntrain_data_config:\n\n    Configuration object for the training data.\n\nvalidate_data_config (default = None):\n\n    Configuration object for the validation data.\n\ndevice (default = 'cuda:0'):\n\n    The device to train on.\n    Set to 'cpu' to train without GPU.\n</code></pre> Source code in <code>cellulus/configs/train_config.py</code> <pre><code>@attrs.define\nclass TrainConfig:\n    \"\"\"Train configuration.\n\n    Parameters:\n\n        crop_size:\n\n            The size of the crops - specified as a list of number of pixels -\n            extracted from the raw images, used during training.\n\n        batch_size:\n\n            The number of samples to use per batch.\n\n        max_iterations:\n\n            The maximum number of iterations to train for.\n\n        initial_learning_rate (default = 4e-5):\n\n            Initial learning rate of the optimizer.\n\n        temperature (default = 10):\n\n            Factor used to scale the gaussian function and control the rate of damping.\n\n        regularizer_weight (default = 1e-5):\n\n            The weight of the L2 regularizer on the object-centric embeddings.\n\n        reduce_mean (default = True):\n\n            If True, the loss contribution is averaged across all pairs of patches.\n\n        density (default = 0.1)\n\n            Determines the fraction of patches to sample per crop, during training.\n\n        kappa (default = 10.0):\n\n            Neighborhood radius to extract patches from.\n\n        save_model_every (default = 1e3):\n\n            The model weights are saved every few iterations.\n\n        save_snapshot_every (default = 1e3):\n\n            The zarr snapshot is saved every few iterations.\n\n        num_workers (default = 8):\n\n            The number of sub-processes to use for data-loading.\n\n        control_point_spacing (default = 64):\n\n            The distance in pixels between control points used for elastic\n            deformation of the raw data during training.\n\n        control_point_jitter (default = 2.0):\n\n            How much to jitter the control points for elastic deformation\n            of the raw data during training, given as the standard deviation of\n            a normal distribution with zero mean.\n\n        train_data_config:\n\n            Configuration object for the training data.\n\n        validate_data_config (default = None):\n\n            Configuration object for the validation data.\n\n        device (default = 'cuda:0'):\n\n            The device to train on.\n            Set to 'cpu' to train without GPU.\n\n\n    \"\"\"\n\n    train_data_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n    validate_data_config: DatasetConfig = attrs.field(\n        default=None, converter=to_config(DatasetConfig)\n    )\n    crop_size: List = attrs.field(default=[252, 252], validator=instance_of(List))\n    batch_size: int = attrs.field(default=8, validator=instance_of(int))\n    max_iterations: int = attrs.field(default=100_000, validator=instance_of(int))\n    initial_learning_rate: float = attrs.field(\n        default=4e-5, validator=instance_of(float)\n    )\n    density: float = attrs.field(default=0.1, validator=instance_of(float))\n    kappa: float = attrs.field(default=10.0, validator=instance_of(float))\n    temperature: float = attrs.field(default=10.0, validator=instance_of(float))\n    regularizer_weight: float = attrs.field(default=1e-5, validator=instance_of(float))\n    reduce_mean: bool = attrs.field(default=True, validator=instance_of(bool))\n    save_model_every: int = attrs.field(default=1_000, validator=instance_of(int))\n    save_snapshot_every: int = attrs.field(default=1_000, validator=instance_of(int))\n    num_workers: int = attrs.field(default=8, validator=instance_of(int))\n\n    control_point_spacing: int = attrs.field(default=64, validator=instance_of(int))\n    control_point_jitter: float = attrs.field(default=2.0, validator=instance_of(float))\n    device: str = attrs.field(default=\"cuda:0\", validator=instance_of(str))\n</code></pre>"},{"location":"api/zarr_dataset/","title":"ZarrDataset","text":"<p>             Bases: <code>IterableDataset</code></p> Source code in <code>cellulus/datasets/zarr_dataset.py</code> <pre><code>class ZarrDataset(IterableDataset):  # type: ignore\n    def __init__(\n        self,\n        dataset_config: DatasetConfig,\n        crop_size: Tuple[int, ...],\n        control_point_spacing: int,\n        control_point_jitter: float,\n    ):\n        \"\"\"A dataset that serves random samples from a zarr container.\n\n        Args:\n\n            dataset_config:\n\n                A dataset config object pointing to the zarr dataset to use.\n                The dataset should have shape `(s, c, [t,] [z,] y, x)`, where\n                `s` = # of samples, `c` = # of channels, `t` = # of frames, and\n                `z`/`y`/`x` are spatial extents. The dataset should have an\n                `\"axis_names\"` attribute that contains the names of the used\n                axes, e.g., `[\"s\", \"c\", \"y\", \"x\"]` for a 2D dataset.\n\n\n            crop_size:\n\n                The size of data crops used during training (distinct from the\n                \"patch size\" of the method: from each crop, multiple patches\n                will be randomly selected and the loss computed on them). This\n                should be equal to the input size of the model that predicts\n                the OCEs.\n\n            control_point_spacing:\n\n                The distance in pixels between control points used for elastic\n                deformation of the raw data.\n\n            control_point_jitter:\n\n                How much to jitter the control points for elastic deformation\n                of the raw data, given as the standard deviation of a normal\n                distribution with zero mean.\n        \"\"\"\n\n        self.dataset_config = dataset_config\n        self.crop_size = crop_size\n        self.control_point_spacing = control_point_spacing\n        self.control_point_jitter = control_point_jitter\n        self.__read_meta_data()\n\n        assert len(crop_size) == self.num_spatial_dims, (\n            f'\"crop_size\" must have the same dimension as the '\n            f'spatial(temporal) dimensions of the \"{self.dataset_config.dataset_name}\" '\n            f\"dataset which is {self.num_spatial_dims}, but it is {crop_size}\"\n        )\n\n        self.__setup_pipeline()\n\n    def __iter__(self):\n        return iter(self.__yield_sample())\n\n    def __setup_pipeline(self):\n        self.raw = gp.ArrayKey(\"RAW\")\n\n        # treat all dimensions as spatial, with a voxel size of 1\n        raw_spec = gp.ArraySpec(voxel_size=(1,) * self.num_dims, interpolatable=True)\n\n        # spatial_dims = tuple(range(self.num_dims - self.num_spatial_dims,\n        # self.num_dims))\n\n        self.pipeline = (\n            gp.ZarrSource(\n                self.dataset_config.container_path,\n                {self.raw: self.dataset_config.dataset_name},\n                array_specs={self.raw: raw_spec},\n            )\n            + gp.RandomLocation()\n            + gp.ElasticAugment(\n                control_point_spacing=(self.control_point_spacing,)\n                * self.num_spatial_dims,\n                jitter_sigma=(self.control_point_jitter,) * self.num_spatial_dims,\n                rotation_interval=(0, math.pi / 2),\n                scale_interval=(0.9, 1.1),\n                subsample=4,\n                spatial_dims=self.num_spatial_dims,\n            )\n            # + gp.SimpleAugment(mirror_only=spatial_dims, transpose_only=spatial_dims)\n        )\n\n    def __yield_sample(self):\n        \"\"\"An infinite generator of crops.\"\"\"\n\n        with gp.build(self.pipeline):\n            while True:\n                # request one sample, all channels, plus crop dimensions\n                request = gp.BatchRequest()\n                request[self.raw] = gp.ArraySpec(\n                    roi=gp.Roi(\n                        (0,) * self.num_dims, (1, self.num_channels, *self.crop_size)\n                    )\n                )\n\n                sample = self.pipeline.request_batch(request)\n                yield sample[self.raw].data[0]\n\n    def __read_meta_data(self):\n        meta_data = DatasetMetaData.from_dataset_config(self.dataset_config)\n\n        self.num_dims = meta_data.num_dims\n        self.num_spatial_dims = meta_data.num_spatial_dims\n        self.num_channels = meta_data.num_channels\n        self.num_samples = meta_data.num_samples\n        self.sample_dim = meta_data.sample_dim\n        self.channel_dim = meta_data.channel_dim\n        self.time_dim = meta_data.time_dim\n\n    def get_num_channels(self):\n        return self.num_channels\n\n    def get_num_spatial_dims(self):\n        return self.num_spatial_dims\n</code></pre>"},{"location":"api/zarr_dataset/#cellulus.datasets.zarr_dataset.ZarrDataset.__init__","title":"<code>__init__(dataset_config, crop_size, control_point_spacing, control_point_jitter)</code>","text":"<p>A dataset that serves random samples from a zarr container.</p> <p>Args:</p> <pre><code>dataset_config:\n\n    A dataset config object pointing to the zarr dataset to use.\n    The dataset should have shape `(s, c, [t,] [z,] y, x)`, where\n    `s` = # of samples, `c` = # of channels, `t` = # of frames, and\n    `z`/`y`/`x` are spatial extents. The dataset should have an\n    `\"axis_names\"` attribute that contains the names of the used\n    axes, e.g., `[\"s\", \"c\", \"y\", \"x\"]` for a 2D dataset.\n\n\ncrop_size:\n\n    The size of data crops used during training (distinct from the\n    \"patch size\" of the method: from each crop, multiple patches\n    will be randomly selected and the loss computed on them). This\n    should be equal to the input size of the model that predicts\n    the OCEs.\n\ncontrol_point_spacing:\n\n    The distance in pixels between control points used for elastic\n    deformation of the raw data.\n\ncontrol_point_jitter:\n\n    How much to jitter the control points for elastic deformation\n    of the raw data, given as the standard deviation of a normal\n    distribution with zero mean.\n</code></pre> Source code in <code>cellulus/datasets/zarr_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_config: DatasetConfig,\n    crop_size: Tuple[int, ...],\n    control_point_spacing: int,\n    control_point_jitter: float,\n):\n    \"\"\"A dataset that serves random samples from a zarr container.\n\n    Args:\n\n        dataset_config:\n\n            A dataset config object pointing to the zarr dataset to use.\n            The dataset should have shape `(s, c, [t,] [z,] y, x)`, where\n            `s` = # of samples, `c` = # of channels, `t` = # of frames, and\n            `z`/`y`/`x` are spatial extents. The dataset should have an\n            `\"axis_names\"` attribute that contains the names of the used\n            axes, e.g., `[\"s\", \"c\", \"y\", \"x\"]` for a 2D dataset.\n\n\n        crop_size:\n\n            The size of data crops used during training (distinct from the\n            \"patch size\" of the method: from each crop, multiple patches\n            will be randomly selected and the loss computed on them). This\n            should be equal to the input size of the model that predicts\n            the OCEs.\n\n        control_point_spacing:\n\n            The distance in pixels between control points used for elastic\n            deformation of the raw data.\n\n        control_point_jitter:\n\n            How much to jitter the control points for elastic deformation\n            of the raw data, given as the standard deviation of a normal\n            distribution with zero mean.\n    \"\"\"\n\n    self.dataset_config = dataset_config\n    self.crop_size = crop_size\n    self.control_point_spacing = control_point_spacing\n    self.control_point_jitter = control_point_jitter\n    self.__read_meta_data()\n\n    assert len(crop_size) == self.num_spatial_dims, (\n        f'\"crop_size\" must have the same dimension as the '\n        f'spatial(temporal) dimensions of the \"{self.dataset_config.dataset_name}\" '\n        f\"dataset which is {self.num_spatial_dims}, but it is {crop_size}\"\n    )\n\n    self.__setup_pipeline()\n</code></pre>"},{"location":"api/zarr_dataset/#cellulus.datasets.zarr_dataset.ZarrDataset.__yield_sample","title":"<code>__yield_sample()</code>","text":"<p>An infinite generator of crops.</p> Source code in <code>cellulus/datasets/zarr_dataset.py</code> <pre><code>def __yield_sample(self):\n    \"\"\"An infinite generator of crops.\"\"\"\n\n    with gp.build(self.pipeline):\n        while True:\n            # request one sample, all channels, plus crop dimensions\n            request = gp.BatchRequest()\n            request[self.raw] = gp.ArraySpec(\n                roi=gp.Roi(\n                    (0,) * self.num_dims, (1, self.num_channels, *self.crop_size)\n                )\n            )\n\n            sample = self.pipeline.request_batch(request)\n            yield sample[self.raw].data[0]\n</code></pre>"},{"location":"examples/2d/01-data/","title":"Download Data","text":"<p>In this notebook, we will download data and convert it to a zarr dataset.</p> <p>For demonstration, we will use a subset of images of <code>Fluo-N2DL-HeLa</code> available on the Cell Tracking Challenge webpage.</p> <p>Firstly, the <code>tif</code> raw images are downloaded to a directory indicated by <code>data_dir</code>.</p> In\u00a0[1]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path In\u00a0[2]: Copied! <pre>import numpy as np\nimport tifffile\nimport zarr\nfrom cellulus.utils.misc import extract_data\nfrom csbdeep.utils import normalize\nfrom tqdm import tqdm\n</pre> import numpy as np import tifffile import zarr from cellulus.utils.misc import extract_data from csbdeep.utils import normalize from tqdm import tqdm In\u00a0[3]: Copied! <pre>name = \"2d-data-demo\"\ndata_dir = \"./data\"\n\nextract_data(\n    zip_url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-data-demo.zip\",\n    data_dir=data_dir,\n    project_name=name,\n)\n</pre> name = \"2d-data-demo\" data_dir = \"./data\"  extract_data(     zip_url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-data-demo.zip\",     data_dir=data_dir,     project_name=name, ) <pre>Directory already exists at the location ./data/2d-data-demo\n</pre> <p>Next, these raw images are intensity-normalized and appended in a list. Here, we use the percentile normalization technique.</p> In\u00a0[4]: Copied! <pre>container_path = zarr.open(name + \".zarr\")\ndataset_name = \"train/raw\"\nimage_filenames = sorted((Path(data_dir) / name / \"images\").glob(\"*.tif\"))\nprint(f\"Number of raw images is {len(image_filenames)}\")\nimage_list = []\n\nfor i in tqdm(range(len(image_filenames))):\n    im = normalize(\n        tifffile.imread(image_filenames[i]).astype(np.float32),\n        pmin=1,\n        pmax=99.8,\n        axis=(0, 1),\n    )\n    image_list.append(im[np.newaxis, ...])\n\nimage_list = np.asarray(image_list)\n</pre> container_path = zarr.open(name + \".zarr\") dataset_name = \"train/raw\" image_filenames = sorted((Path(data_dir) / name / \"images\").glob(\"*.tif\")) print(f\"Number of raw images is {len(image_filenames)}\") image_list = []  for i in tqdm(range(len(image_filenames))):     im = normalize(         tifffile.imread(image_filenames[i]).astype(np.float32),         pmin=1,         pmax=99.8,         axis=(0, 1),     )     image_list.append(im[np.newaxis, ...])  image_list = np.asarray(image_list) <pre>Number of raw images is 11\n</pre> <pre>\r  0%|                                                                                                           | 0/11 [00:00&lt;?, ?it/s]</pre> <pre>\r 91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588         | 10/11 [00:00&lt;00:00, 95.03it/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11/11 [00:00&lt;00:00, 94.49it/s]</pre> <pre>\n</pre> <p>Lastly, the zarr dataset is populated, the axis names and resolution is specified.</p> In\u00a0[5]: Copied! <pre>container_path[dataset_name] = image_list\ncontainer_path[dataset_name].attrs[\"resolution\"] = (1, 1)\ncontainer_path[dataset_name].attrs[\"axis_names\"] = (\"s\", \"c\", \"y\", \"x\")\n</pre> container_path[dataset_name] = image_list container_path[dataset_name].attrs[\"resolution\"] = (1, 1) container_path[dataset_name].attrs[\"axis_names\"] = (\"s\", \"c\", \"y\", \"x\")"},{"location":"examples/2d/01-data/#download-data","title":"Download Data\u00b6","text":""},{"location":"examples/2d/02-train/","title":"Train Model","text":"<p>In this notebook, we will train a <code>cellulus</code> model.</p> In\u00a0[1]: Copied! <pre>from attrs import asdict\nfrom cellulus.configs.dataset_config import DatasetConfig\nfrom cellulus.configs.experiment_config import ExperimentConfig\nfrom cellulus.configs.model_config import ModelConfig\nfrom cellulus.configs.train_config import TrainConfig\n</pre> from attrs import asdict from cellulus.configs.dataset_config import DatasetConfig from cellulus.configs.experiment_config import ExperimentConfig from cellulus.configs.model_config import ModelConfig from cellulus.configs.train_config import TrainConfig <p>In the next cell, we specify the name of the zarr container and the dataset within it from which data would be read.</p> In\u00a0[2]: Copied! <pre>name = \"2d-data-demo\"\ndataset_name = \"train/raw\"\n</pre> name = \"2d-data-demo\" dataset_name = \"train/raw\" In\u00a0[3]: Copied! <pre>train_data_config = DatasetConfig(\n    container_path=name + \".zarr\", dataset_name=dataset_name\n)\n</pre> train_data_config = DatasetConfig(     container_path=name + \".zarr\", dataset_name=dataset_name ) <p>In the next cell, we specify the number of feature maps (<code>num_fmaps</code>) in the first layer in our model.  Additionally, we specify <code>fmap_inc_factor</code>, which indicates by how much the number of feature maps increase between adjacent layers.</p> In\u00a0[4]: Copied! <pre>num_fmaps = 24\nfmap_inc_factor = 3\n</pre> num_fmaps = 24 fmap_inc_factor = 3 In\u00a0[5]: Copied! <pre>model_config = ModelConfig(num_fmaps=num_fmaps, fmap_inc_factor=fmap_inc_factor)\n</pre> model_config = ModelConfig(num_fmaps=num_fmaps, fmap_inc_factor=fmap_inc_factor) <p>Then, we specify training-specific parameters such as the <code>device</code>, which indicates the actual device to run the training on.  The device could be set equal to <code>cuda:n</code> (where <code>n</code> is the index of the GPU, for e.g. <code>cuda:0</code>), <code>cpu</code> or <code>mps</code>.  We set the <code>num_iterations</code> equal to <code>5e3</code> for demonstration purposes. (This takes around 20 minutes on a Mac Book Pro with an Apple M2 Max chip).</p> In\u00a0[6]: Copied! <pre>device = \"mps\"\nnum_iterations = 5e3\n</pre> device = \"mps\" num_iterations = 5e3 In\u00a0[7]: Copied! <pre>train_config = TrainConfig(\n    train_data_config=asdict(train_data_config),\n    device=device,\n    num_iterations=num_iterations,\n)\n</pre> train_config = TrainConfig(     train_data_config=asdict(train_data_config),     device=device,     num_iterations=num_iterations, ) <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[7], line 1\n----&gt; 1 train_config = TrainConfig(\n      2     train_data_config=asdict(train_data_config),\n      3     device=device,\n      4     num_iterations=num_iterations,\n      5 )\n\nTypeError: __init__() got an unexpected keyword argument 'num_iterations'</pre> <p>Next, we initialize the experiment config which puts together the config objects (<code>train_config</code> and <code>model_config</code>) which we defined above.</p> In\u00a0[8]: Copied! <pre>experiment_config = ExperimentConfig(\n    train_config=asdict(train_config), model_config=asdict(model_config)\n)\n</pre> experiment_config = ExperimentConfig(     train_config=asdict(train_config), model_config=asdict(model_config) ) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[8], line 2\n      1 experiment_config = ExperimentConfig(\n----&gt; 2     train_config=asdict(train_config), model_config=asdict(model_config)\n      3 )\n\nNameError: name 'train_config' is not defined</pre> <p>Now we can begin the training!  Uncomment the next two lines to train the model.</p> In\u00a0[9]: Copied! <pre># from cellulus.train import train\n# train(experiment_config)\n</pre> # from cellulus.train import train # train(experiment_config)"},{"location":"examples/2d/02-train/#train-model","title":"Train Model\u00b6","text":""},{"location":"examples/2d/02-train/#specify-config-values-for-dataset","title":"Specify config values for dataset\u00b6","text":""},{"location":"examples/2d/02-train/#specify-config-values-for-model","title":"Specify config values for model\u00b6","text":""},{"location":"examples/2d/02-train/#specify-config-values-for-the-training-process","title":"Specify config values for the training process\u00b6","text":""},{"location":"examples/2d/03-infer/","title":"Infer using Trained Model","text":"<p>In this notebook, we will use the <code>cellulus</code> model trained in the previous step to obtain instance segmentations.</p> In\u00a0[1]: Copied! <pre>import zipfile\n</pre> import zipfile In\u00a0[2]: Copied! <pre>import skimage\nimport torch\nimport zarr\nfrom attrs import asdict\nfrom cellulus.configs.dataset_config import DatasetConfig\nfrom cellulus.configs.experiment_config import ExperimentConfig\nfrom cellulus.configs.inference_config import InferenceConfig\nfrom cellulus.configs.model_config import ModelConfig\nfrom cellulus.infer import infer\nfrom cellulus.utils.misc import visualize_2d\n</pre> import skimage import torch import zarr from attrs import asdict from cellulus.configs.dataset_config import DatasetConfig from cellulus.configs.experiment_config import ExperimentConfig from cellulus.configs.inference_config import InferenceConfig from cellulus.configs.model_config import ModelConfig from cellulus.infer import infer from cellulus.utils.misc import visualize_2d <p>We again specify <code>name</code> of the zarr container, and <code>dataset_name</code> which identifies the path to the raw image data, which needs to be segmented.</p> In\u00a0[3]: Copied! <pre>name = \"2d-data-demo\"\ndataset_name = \"train/raw\"\n</pre> name = \"2d-data-demo\" dataset_name = \"train/raw\" <p>We initialize the <code>dataset_config</code> which relates to the raw image data, <code>prediction_dataset_config</code> which relates to the per-pixel embeddings and the uncertainty, the <code>segmentation_dataset_config</code> which relates to the segmentations post the mean-shift clustering and the <code>post_processed_config</code> which relates to the segmentations after some post-processing.</p> In\u00a0[4]: Copied! <pre>dataset_config = DatasetConfig(container_path=name + \".zarr\", dataset_name=dataset_name)\nprediction_dataset_config = DatasetConfig(\n    container_path=name + \".zarr\", dataset_name=\"embeddings\"\n)\nsegmentation_dataset_config = DatasetConfig(\n    container_path=name + \".zarr\",\n    dataset_name=\"segmentation\",\n    secondary_dataset_name=\"embeddings\",\n)\npost_processed_dataset_config = DatasetConfig(\n    container_path=name + \".zarr\",\n    dataset_name=\"post_processed_segmentation\",\n    secondary_dataset_name=\"segmentation\",\n)\n</pre> dataset_config = DatasetConfig(container_path=name + \".zarr\", dataset_name=dataset_name) prediction_dataset_config = DatasetConfig(     container_path=name + \".zarr\", dataset_name=\"embeddings\" ) segmentation_dataset_config = DatasetConfig(     container_path=name + \".zarr\",     dataset_name=\"segmentation\",     secondary_dataset_name=\"embeddings\", ) post_processed_dataset_config = DatasetConfig(     container_path=name + \".zarr\",     dataset_name=\"post_processed_segmentation\",     secondary_dataset_name=\"segmentation\", ) <p>We must also specify the <code>num_fmaps</code>, <code>fmap_inc_factor</code> (use same values as in the training step) and set <code>checkpoint</code> equal to <code>models/best_loss.pth</code> (best in terms of the lowest loss obtained).</p> <p>Here, we download a pretrained model trained by us for <code>5e3</code> iterations.  But please comment the next cell to use your own trained model, which should be available in the <code>models</code> directory.</p> In\u00a0[5]: Copied! <pre>torch.hub.download_url_to_file(\n    url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-demo-model.zip\",\n    dst=\"pretrained_model\",\n    progress=True,\n)\nwith zipfile.ZipFile(\"pretrained_model\", \"r\") as zip_ref:\n    zip_ref.extractall(\"\")\n</pre> torch.hub.download_url_to_file(     url=\"https://github.com/funkelab/cellulus/releases/download/v0.0.1-tag/2d-demo-model.zip\",     dst=\"pretrained_model\",     progress=True, ) with zipfile.ZipFile(\"pretrained_model\", \"r\") as zip_ref:     zip_ref.extractall(\"\") <pre>\r  0%|                                                                                                      | 0.00/1.96M [00:00&lt;?, ?B/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.96M/1.96M [00:00&lt;00:00, 20.2MB/s]</pre> <pre>\r100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.96M/1.96M [00:00&lt;00:00, 19.6MB/s]</pre> <pre>\n</pre> In\u00a0[6]: Copied! <pre>num_fmaps = 24\nfmap_inc_factor = 3\ncheckpoint = \"models/best_loss.pth\"\n</pre> num_fmaps = 24 fmap_inc_factor = 3 checkpoint = \"models/best_loss.pth\" In\u00a0[7]: Copied! <pre>model_config = ModelConfig(\n    num_fmaps=num_fmaps, fmap_inc_factor=fmap_inc_factor, checkpoint=checkpoint\n)\n</pre> model_config = ModelConfig(     num_fmaps=num_fmaps, fmap_inc_factor=fmap_inc_factor, checkpoint=checkpoint ) <p>Then, we specify inference-specific parameters such as the <code>device</code>, which indicates the actual device to run the inference on.  The device could be set equal to <code>cuda:n</code> (where <code>n</code> is the index of the GPU, for e.g. <code>cuda:0</code>), <code>cpu</code> or <code>mps</code>.</p> In\u00a0[8]: Copied! <pre>device = \"mps\"\n</pre> device = \"mps\" <p>We initialize the <code>inference_config</code> which contains our <code>embeddings_dataset_config</code>, <code>segmentation_dataset_config</code> and <code>post_processed_dataset_config</code>.</p> In\u00a0[9]: Copied! <pre>inference_config = InferenceConfig(\n    dataset_config=asdict(dataset_config),\n    prediction_dataset_config=asdict(prediction_dataset_config),\n    segmentation_dataset_config=asdict(segmentation_dataset_config),\n    post_processed_dataset_config=asdict(post_processed_dataset_config),\n    device=device,\n)\n</pre> inference_config = InferenceConfig(     dataset_config=asdict(dataset_config),     prediction_dataset_config=asdict(prediction_dataset_config),     segmentation_dataset_config=asdict(segmentation_dataset_config),     post_processed_dataset_config=asdict(post_processed_dataset_config),     device=device, ) <p>Lastly we initialize the <code>experiment_config</code> which contains the <code>inference_config</code> and <code>model_config</code> initialized above.</p> In\u00a0[10]: Copied! <pre>experiment_config = ExperimentConfig(\n    inference_config=asdict(inference_config), model_config=asdict(model_config)\n)\n</pre> experiment_config = ExperimentConfig(     inference_config=asdict(inference_config), model_config=asdict(model_config) ) <p>Now we are ready to start the inference!!  (This takes around 7 minutes on a Mac Book Pro with an Apple M2 Max chip).</p> In\u00a0[11]: Copied! <pre>infer(experiment_config)\n</pre> infer(experiment_config) <pre>ExperimentConfig(model_config=ModelConfig(num_fmaps=24, fmap_inc_factor=3, features_in_last_layer=64, downsampling_factors=[[2, 2]], checkpoint=PosixPath('models/best_loss.pth'), initialize=True), experiment_name='2024-01-09', object_size=26.0, train_config=None, inference_config=InferenceConfig(dataset_config=DatasetConfig(container_path=PosixPath('2d-data-demo.zarr'), dataset_name='train/raw', secondary_dataset_name=None), prediction_dataset_config=DatasetConfig(container_path=PosixPath('2d-data-demo.zarr'), dataset_name='embeddings', secondary_dataset_name=None), segmentation_dataset_config=DatasetConfig(container_path=PosixPath('2d-data-demo.zarr'), dataset_name='segmentation', secondary_dataset_name='embeddings'), post_processed_dataset_config=DatasetConfig(container_path=PosixPath('2d-data-demo.zarr'), dataset_name='post_processed_segmentation', secondary_dataset_name='segmentation'), evaluation_dataset_config=None, device='mps', crop_size=[252, 252], p_salt_pepper=0.01, num_infer_iterations=16, threshold=None, bandwidth=None, num_bandwidths=1, reduction_probability=0.1, min_size=None, grow_distance=3, shrink_distance=6))\n</pre> <pre>\n---------------------------------------------------------------------------\nContainsArrayError                        Traceback (most recent call last)\nCell In[11], line 1\n----&gt; 1 infer(experiment_config)\n\nFile ~/Desktop/github/cellulus/cellulus/infer.py:70, in infer(experiment_config)\n     68 # get predicted embeddings...\n     69 if inference_config.prediction_dataset_config is not None:\n---&gt; 70     predict(model, inference_config)\n     71 # ...turn them into a segmentation...\n     72 if inference_config.segmentation_dataset_config is not None:\n\nFile ~/Desktop/github/cellulus/cellulus/predict.py:100, in predict(model, inference_config)\n     98 # prepare the zarr dataset to write to\n     99 f = zarr.open(inference_config.prediction_dataset_config.container_path)\n--&gt; 100 ds = f.create_dataset(\n    101     inference_config.prediction_dataset_config.dataset_name,\n    102     shape=(\n    103         dataset_meta_data.num_samples,\n    104         dataset_meta_data.num_spatial_dims + 1,\n    105         *dataset_meta_data.spatial_array,\n    106     ),\n    107     dtype=float,\n    108 )\n    110 pipeline = (\n    111     gp.ZarrSource(\n    112         dataset_config.container_path,\n   (...)\n    124     + gp.Scan(scan_request)\n    125 )\n    127 request = gp.BatchRequest()\n\nFile ~/miniforge3/envs/cellulus/lib/python3.9/site-packages/zarr/hierarchy.py:1094, in Group.create_dataset(self, name, **kwargs)\n   1035 \"\"\"Create an array.\n   1036 \n   1037 Arrays are known as \"datasets\" in HDF5 terminology. For compatibility\n   (...)\n   1090 \n   1091 \"\"\"\n   1092 assert \"mode\" not in kwargs\n-&gt; 1094 return self._write_op(self._create_dataset_nosync, name, **kwargs)\n\nFile ~/miniforge3/envs/cellulus/lib/python3.9/site-packages/zarr/hierarchy.py:935, in Group._write_op(self, f, *args, **kwargs)\n    932     lock = self._synchronizer[group_meta_key]\n    934 with lock:\n--&gt; 935     return f(*args, **kwargs)\n\nFile ~/miniforge3/envs/cellulus/lib/python3.9/site-packages/zarr/hierarchy.py:1107, in Group._create_dataset_nosync(self, name, data, **kwargs)\n   1105 # create array\n   1106 if data is None:\n-&gt; 1107     a = create(store=self._store, path=path, chunk_store=self._chunk_store, **kwargs)\n   1109 else:\n   1110     a = array(data, store=self._store, path=path, chunk_store=self._chunk_store, **kwargs)\n\nFile ~/miniforge3/envs/cellulus/lib/python3.9/site-packages/zarr/creation.py:203, in create(shape, chunks, dtype, compressor, fill_value, order, store, synchronizer, overwrite, path, chunk_store, filters, cache_metadata, cache_attrs, read_only, object_codec, dimension_separator, write_empty_chunks, zarr_version, meta_array, storage_transformers, **kwargs)\n    200     path = \"/\"\n    202 # initialize array metadata\n--&gt; 203 init_array(\n    204     store,\n    205     shape=shape,\n    206     chunks=chunks,\n    207     dtype=dtype,\n    208     compressor=compressor,\n    209     fill_value=fill_value,\n    210     order=order,\n    211     overwrite=overwrite,\n    212     path=path,\n    213     chunk_store=chunk_store,\n    214     filters=filters,\n    215     object_codec=object_codec,\n    216     dimension_separator=dimension_separator,\n    217     storage_transformers=storage_transformers,\n    218 )\n    220 # instantiate array\n    221 z = Array(\n    222     store,\n    223     path=path,\n   (...)\n    230     meta_array=meta_array,\n    231 )\n\nFile ~/miniforge3/envs/cellulus/lib/python3.9/site-packages/zarr/storage.py:451, in init_array(store, shape, chunks, dtype, compressor, fill_value, order, overwrite, path, chunk_store, filters, object_codec, dimension_separator, storage_transformers)\n    448 if not compressor:\n    449     # compatibility with legacy tests using compressor=[]\n    450     compressor = None\n--&gt; 451 _init_array_metadata(\n    452     store,\n    453     shape=shape,\n    454     chunks=chunks,\n    455     dtype=dtype,\n    456     compressor=compressor,\n    457     fill_value=fill_value,\n    458     order=order,\n    459     overwrite=overwrite,\n    460     path=path,\n    461     chunk_store=chunk_store,\n    462     filters=filters,\n    463     object_codec=object_codec,\n    464     dimension_separator=dimension_separator,\n    465     storage_transformers=storage_transformers,\n    466 )\n\nFile ~/miniforge3/envs/cellulus/lib/python3.9/site-packages/zarr/storage.py:521, in _init_array_metadata(store, shape, chunks, dtype, compressor, fill_value, order, overwrite, path, chunk_store, filters, object_codec, dimension_separator, storage_transformers)\n    519 if not overwrite:\n    520     if contains_array(store, path):\n--&gt; 521         raise ContainsArrayError(path)\n    522     elif contains_group(store, path, explicit_only=False):\n    523         raise ContainsGroupError(path)\n\nContainsArrayError: path 'embeddings' contains an array</pre> <p>Let's look at some of the predicted embeddings.  Change the value of <code>index</code> below to look at the raw image (left), x-offset (bottom-left), y-offset (bottom-right) and uncertainty of the embedding (top-right).</p> In\u00a0[12]: Copied! <pre>index = 0\n\nf = zarr.open(name + \".zarr\")\nds = f[\"train/raw\"]\nds2 = f[\"centered_embeddings\"]\n\nimage = ds[index, 0]\nembedding = ds2[index]\n\nvisualize_2d(\n    image,\n    top_right=embedding[-1],\n    bottom_left=embedding[0],\n    bottom_right=embedding[1],\n    top_right_label=\"STD_DEV\",\n    bottom_left_label=\"OFFSET_X\",\n    bottom_right_label=\"OFFSET_Y\",\n)\n</pre> index = 0  f = zarr.open(name + \".zarr\") ds = f[\"train/raw\"] ds2 = f[\"centered_embeddings\"]  image = ds[index, 0] embedding = ds2[index]  visualize_2d(     image,     top_right=embedding[-1],     bottom_left=embedding[0],     bottom_right=embedding[1],     top_right_label=\"STD_DEV\",     bottom_left_label=\"OFFSET_X\",     bottom_right_label=\"OFFSET_Y\", ) <p>As you can see the magnitude of the uncertainty of the embedding (top-right) is low for most of the foreground cells.  This enables extraction of the foreground, which is eventually clustered into individual instances.</p> In\u00a0[13]: Copied! <pre>index = 0\n\nf = zarr.open(name + \".zarr\")\nds = f[\"train/raw\"]\nds2 = f[\"segmentation\"]\nds3 = f[\"post_processed_segmentation\"]\n\nvisualize_2d(\n    image,\n    top_right=embedding[-1] &lt; skimage.filters.threshold_otsu(embedding[-1]),\n    bottom_left=ds2[0, 0],\n    bottom_right=ds3[0, 0],\n    top_right_label=\"THRESHOLDED F.G.\",\n    bottom_left_label=\"SEGMENTATION\",\n    bottom_right_label=\"POSTPROCESSED\",\n    top_right_cmap=\"gray\",\n    bottom_left_cmap=\"nipy_spectral\",\n    bottom_right_cmap=\"nipy_spectral\",\n)\n</pre> index = 0  f = zarr.open(name + \".zarr\") ds = f[\"train/raw\"] ds2 = f[\"segmentation\"] ds3 = f[\"post_processed_segmentation\"]  visualize_2d(     image,     top_right=embedding[-1] &lt; skimage.filters.threshold_otsu(embedding[-1]),     bottom_left=ds2[0, 0],     bottom_right=ds3[0, 0],     top_right_label=\"THRESHOLDED F.G.\",     bottom_left_label=\"SEGMENTATION\",     bottom_right_label=\"POSTPROCESSED\",     top_right_cmap=\"gray\",     bottom_left_cmap=\"nipy_spectral\",     bottom_right_cmap=\"nipy_spectral\", )"},{"location":"examples/2d/03-infer/#infer-using-trained-model","title":"Infer using Trained Model\u00b6","text":""},{"location":"examples/2d/03-infer/#specify-config-values-for-datasets","title":"Specify config values for datasets\u00b6","text":""},{"location":"examples/2d/03-infer/#specify-config-values-for-the-model","title":"Specify config values for the model\u00b6","text":""},{"location":"examples/2d/03-infer/#initialize-inference_config","title":"Initialize <code>inference_config</code>\u00b6","text":""},{"location":"examples/2d/03-infer/#initialize-experiment_config","title":"Initialize <code>experiment_config</code>\u00b6","text":""},{"location":"examples/2d/03-infer/#inspect-predictions","title":"Inspect predictions\u00b6","text":""}]}